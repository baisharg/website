<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1.0"
		/>
		<title>Chapter 5: Superposition - BAISH Mech Interp Handbook</title>
		<meta
			name="description"
			content="Superposition in Neural Networks - BAISH Mechanistic Interpretability Handbook"
		/>
		<!-- Favicon -->
		<link rel="icon" href="../img/favicon.ico" />
		<!-- Google Fonts -->
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&family=Playfair+Display:wght@400;600&display=swap"
			rel="stylesheet"
		/>
		<!-- Custom CSS -->
		<link rel="stylesheet" href="../css/style.css" />
		<link rel="stylesheet" href="../css/language-toggle.css" />
	</head>
	<body>
		<a href="#main-content" class="skip-link">Skip to content</a>
		<header class="site-header">
			<div class="container">
				<div class="logo">
					<a href="../index.html">
						<img src="../img/logo.svg" alt="BAISH Logo" class="logo-img" />
						<span class="logo-text">BAISH - Buenos Aires AI Safety Hub</span>
					</a>
				</div>
				<button class="mobile-menu-button" aria-label="Toggle menu">
					<span></span>
					<span></span>
					<span></span>
				</button>
				<nav class="main-nav">
					<ul>
						<li><a href="../about.html" data-i18n="nav.about">About</a></li>
						<li>
							<a href="../activities.html" data-i18n="nav.activities"
								>Activities</a
							>
						</li>
						<li>
							<a href="../research.html" data-i18n="nav.research">Research</a>
						</li>
						<li>
							<a href="../resources.html" data-i18n="nav.resources"
								>Resources</a
							>
						</li>
						<li>
							<a href="../contact.html" data-i18n="nav.contact">Contact</a>
						</li>
					</ul>
				</nav>
				<div class="language-toggle">
					<button id="lang-es" class="language-toggle-btn active">ES</button>
					<span class="language-toggle-separator">|</span>
					<button id="lang-en" class="language-toggle-btn">EN</button>
				</div>
				<a href="../contact.html" class="btn btn-primary" data-i18n="nav.joinUs"
					>Join Us</a
				>
			</div>
		</header>

		<section id="main-content" class="page-header">
			<div class="container">
				<div class="breadcrumb" style="margin-bottom: 1rem">
					<a
						href="../mech-interp-course.html"
						class="btn btn-secondary"
						style="margin-right: 1rem"
						data-i18n="mechInterpCourse.backToHome"
						>← Back to Course</a
					>
				</div>
				<h1 data-i18n="handbook.chapter5.title">Chapter 5: Superposition</h1>
				<p class="subtitle" data-i18n="handbook.chapter5.subtitle">
					Understanding how neural networks represent more features than they
					have dimensions
				</p>
			</div>
		</section>

		<section class="handbook-content">
			<div class="container">
				<div class="handbook-navigation">
					<div class="handbook-toc">
						<h3 data-i18n="handbook.tableOfContents">Table of Contents</h3>
						<ul>
							<li>
								<a
									href="#introduction"
									data-i18n="handbook.chapter5.sections.intro"
									>Introduction to Superposition</a
								>
							</li>
							<li>
								<a
									href="#feature-competition"
									data-i18n="handbook.chapter5.sections.competition"
									>Feature Competition</a
								>
							</li>
							<li>
								<a
									href="#polysemanticity"
									data-i18n="handbook.chapter5.sections.polysemanticity"
									>Polysemanticity</a
								>
							</li>
							<li>
								<a
									href="#toy-models"
									data-i18n="handbook.chapter5.sections.toyModels"
									>Toy Models of Superposition</a
								>
							</li>
							<li>
								<a
									href="#implications"
									data-i18n="handbook.chapter5.sections.implications"
									>Implications for Interpretability</a
								>
							</li>
							<li>
								<a
									href="#conclusion"
									data-i18n="handbook.chapter5.sections.conclusion"
									>Conclusion</a
								>
							</li>
						</ul>
					</div>
					<div class="chapter-navigation">
						<a
							href="chapter4.html"
							class="prev-chapter"
							data-i18n="handbook.prevChapter"
							>← Previous Chapter: Circuits</a
						>
						<a
							href="chapter6.html"
							class="next-chapter"
							data-i18n="handbook.nextChapter"
							>Next Chapter: Sparse Autoencoders →</a
						>
					</div>
				</div>

				<div class="handbook-main">
					<h2 id="introduction" data-i18n="handbook.chapter5.sections.intro">
						Introduction to Superposition
					</h2>
					<p data-i18n="handbook.chapter5.intro.p1">
						Superposition refers to the phenomenon where neural networks
						represent more features than they have neurons or dimensions. This
						occurs because networks often need to track many more features than
						they have parameters available to represent them individually.
					</p>
					<p data-i18n="handbook.chapter5.intro.p2">
						The concept was formalized in the paper "Toy Models of
						Superposition" by Anthropic, which demonstrated how networks can
						encode multiple features in a lower-dimensional space by exploiting
						the geometry of feature co-occurrence patterns.
					</p>

					<h2
						id="feature-competition"
						data-i18n="handbook.chapter5.sections.competition"
					>
						Feature Competition
					</h2>
					<p data-i18n="handbook.chapter5.competition.p1">
						When a neural network has fewer dimensions than the features it
						needs to represent, these features must "compete" for representation
						space. The network learns to allocate its limited representational
						capacity efficiently.
					</p>
					<p data-i18n="handbook.chapter5.competition.p2">
						Key factors that influence which features get represented include:
					</p>
					<ul>
						<li data-i18n="handbook.chapter5.competition.list1">
							Frequency: More common features are more likely to be represented
						</li>
						<li data-i18n="handbook.chapter5.competition.list2">
							Importance: Features that strongly affect the loss function are
							prioritized
						</li>
						<li data-i18n="handbook.chapter5.competition.list3">
							Correlation: Features that often co-occur can share representation
							space
						</li>
						<li data-i18n="handbook.chapter5.competition.list4">
							Orthogonality: Features that can be represented in orthogonal
							directions are easier to separate
						</li>
					</ul>

					<h2
						id="polysemanticity"
						data-i18n="handbook.chapter5.sections.polysemanticity"
					>
						Polysemanticity
					</h2>
					<p data-i18n="handbook.chapter5.polysemanticity.p1">
						Polysemanticity is a direct consequence of superposition. It refers
						to the phenomenon where individual neurons or network directions
						respond to multiple unrelated features.
					</p>
					<p data-i18n="handbook.chapter5.polysemanticity.p2">
						In a polysemantic network:
					</p>
					<ul>
						<li data-i18n="handbook.chapter5.polysemanticity.list1">
							Single neurons may respond to multiple semantically distinct
							concepts
						</li>
						<li data-i18n="handbook.chapter5.polysemanticity.list2">
							Feature representations are distributed across many neurons
						</li>
						<li data-i18n="handbook.chapter5.polysemanticity.list3">
							There may not be a clear one-to-one mapping between network
							components and human-interpretable concepts
						</li>
					</ul>
					<p data-i18n="handbook.chapter5.polysemanticity.p3">
						This makes interpretation challenging, as we cannot simply analyze
						individual neurons to understand what the network is representing.
					</p>

					<h2 id="toy-models" data-i18n="handbook.chapter5.sections.toyModels">
						Toy Models of Superposition
					</h2>
					<p data-i18n="handbook.chapter5.toyModels.p1">
						Researchers have developed simplified models to study superposition.
						These toy models help illustrate how networks can embed many
						features in lower-dimensional spaces.
					</p>
					<p data-i18n="handbook.chapter5.toyModels.p2">
						A typical toy model might involve:
					</p>
					<ol>
						<li data-i18n="handbook.chapter5.toyModels.step1">
							Generating synthetic data with a known number of sparse features
						</li>
						<li data-i18n="handbook.chapter5.toyModels.step2">
							Training a model with fewer dimensions than features
						</li>
						<li data-i18n="handbook.chapter5.toyModels.step3">
							Analyzing how the model represents these features in its limited
							space
						</li>
					</ol>
					<p data-i18n="handbook.chapter5.toyModels.p3">
						These experiments have revealed that networks can use clever
						geometric arrangements to encode features efficiently, often
						exploiting properties like sparsity (features rarely appearing
						simultaneously).
					</p>

					<h2
						id="implications"
						data-i18n="handbook.chapter5.sections.implications"
					>
						Implications for Interpretability
					</h2>
					<p data-i18n="handbook.chapter5.implications.p1">
						Superposition poses several challenges for interpretability
						research:
					</p>
					<ul>
						<li data-i18n="handbook.chapter5.implications.list1">
							Direct neuron analysis may reveal misleading or incomplete
							information
						</li>
						<li data-i18n="handbook.chapter5.implications.list2">
							Features may be encoded in complex, distributed patterns across
							many neurons
						</li>
						<li data-i18n="handbook.chapter5.implications.list3">
							Simple linear probing techniques may fail to detect important
							features
						</li>
					</ul>
					<p data-i18n="handbook.chapter5.implications.p2">
						However, understanding superposition also provides opportunities:
					</p>
					<ul>
						<li data-i18n="handbook.chapter5.implications.list4">
							It suggests focusing on finding the right basis for analysis,
							rather than examining individual neurons
						</li>
						<li data-i18n="handbook.chapter5.implications.list5">
							Techniques like Sparse Autoencoders (covered in the next chapter)
							can help extract features from superposed representations
						</li>
						<li data-i18n="handbook.chapter5.implications.list6">
							Knowledge of superposition patterns can inform better training and
							architecture design
						</li>
					</ul>

					<h2 id="conclusion" data-i18n="handbook.chapter5.sections.conclusion">
						Conclusion
					</h2>
					<p data-i18n="handbook.chapter5.conclusion.p1">
						Superposition is a fundamental property of neural networks that
						arises when they need to represent more features than they have
						dimensions. This leads to polysemantic neurons and distributed
						representations that complicate interpretability efforts.
					</p>
					<p data-i18n="handbook.chapter5.conclusion.p2">
						Understanding superposition is essential for developing effective
						methods to interpret neural networks, especially large language
						models with billions of parameters tracking potentially trillions of
						features. The next chapter will explore how Sparse Autoencoders can
						help address this challenge by disentangling these superposed
						representations.
					</p>

					<div class="further-reading">
						<h3 data-i18n="handbook.furtherReading">Further Reading</h3>
						<ul>
							<li>
								<a
									href="https://transformer-circuits.pub/2022/toy_model/index.html"
									target="_blank"
									data-i18n="handbook.chapter5.reading1"
									>Toy Models of Superposition (Anthropic)</a
								>
							</li>
							<li>
								<a
									href="https://arxiv.org/abs/2211.11754"
									target="_blank"
									data-i18n="handbook.chapter5.reading2"
									>Progress Measures for Grokking via Mechanistic
									Interpretability</a
								>
							</li>
							<li>
								<a
									href="https://www.youtube.com/watch?v=mSn7Rr2X8G8"
									target="_blank"
									data-i18n="handbook.chapter5.reading3"
									>3Blue1Brown: How LLMs might store facts?</a
								>
							</li>
						</ul>
					</div>
				</div>
			</div>
		</section>

		<footer class="site-footer">
			<div class="container">
				<div class="footer-content">
					<div class="footer-logo">
						<img
							src="../img/logo.svg"
							alt="BAISH Logo"
							class="footer-logo-img"
						/>
						<p class="tagline" data-i18n="footer.tagline">
							Ensuring AI benefits humanity through research, education, and
							community
						</p>
					</div>
					<div class="footer-links">
						<ul>
							<li><a href="../about.html" data-i18n="nav.about">About</a></li>
							<li>
								<a href="../activities.html" data-i18n="nav.activities"
									>Activities</a
								>
							</li>
							<li>
								<a href="../research.html" data-i18n="nav.research">Research</a>
							</li>
							<li>
								<a href="../resources.html" data-i18n="nav.resources"
									>Resources</a
								>
							</li>
							<li>
								<a href="../contact.html" data-i18n="nav.contact">Contact</a>
							</li>
						</ul>
					</div>
				</div>
				<div class="footer-nav">
					<div class="footer-nav-left">
						<span data-i18n="footer.copyright"
							>&copy; 2024 BAISH - Buenos Aires AI Safety Hub</span
						>
						<a href="../privacy-policy.html" data-i18n="footer.privacy"
							>Privacy Policy</a
						>
					</div>
					<div class="footer-nav-right">
						<div class="social-links">
							<a
								href="https://twitter.com/baish_ai"
								target="_blank"
								aria-label="Twitter"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"
									/>
								</svg>
							</a>
							<a
								href="https://github.com/BAISH-AI"
								target="_blank"
								aria-label="GitHub"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"
									/>
								</svg>
							</a>
							<a
								href="https://www.linkedin.com/company/baish-ai"
								target="_blank"
								aria-label="LinkedIn"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"
									/>
									<rect x="2" y="9" width="4" height="12" />
									<circle cx="4" cy="4" r="2" />
								</svg>
							</a>
						</div>
					</div>
				</div>
			</div>
		</footer>

		<!-- JavaScript -->
		<script src="../js/translations.js"></script>
		<script src="../js/language-toggle.js"></script>
		<script src="../js/mobile-nav.js"></script>
		<script>
			// Force language application on page load
			document.addEventListener("DOMContentLoaded", function () {
				// Apply translations based on stored language
				const storedLang = localStorage.getItem("language") || "es";
				currentLanguage = storedLang;
				updateLanguageToggleUI();
				applyTranslations();
			});
		</script>
	</body>
</html>
