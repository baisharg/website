<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
		/>
		<title>Chapter 2: Transformers - BAISH Mech Interp Handbook</title>
		<meta
			name="description"
			content="Introduction to Transformers - BAISH Mechanistic Interpretability Handbook"
		/>
		<!-- Favicon -->
		<link rel="icon" href="../img/favicon.ico" />
		<!-- Google Fonts -->
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Source+Serif+Pro:wght@400;600&display=swap"
			rel="stylesheet"
		/>
		<!-- Custom CSS -->
		<link rel="stylesheet" href="../css/style.css" />
		<link rel="stylesheet" href="../css/language-toggle.css" />
	</head>
	<body>
		<header class="site-header">
			<div class="container">
				<div class="logo">
					<a href="../index.html">
						<img src="../img/logo.svg" alt="BAISH Logo" class="logo-img" />
						<span class="logo-text">BAISH - Buenos Aires AI Safety Hub</span>
					</a>
				</div>
				<button class="mobile-menu-button" aria-label="Toggle menu">
					<span></span>
					<span></span>
					<span></span>
				</button>
				<nav class="main-nav">
					<ul>
						<li><a href="../about.html" data-i18n="nav.about">About</a></li>
						<li>
							<a href="../activities.html" data-i18n="nav.activities"
								>Activities</a
							>
						</li>
						<li>
							<a href="../research.html" data-i18n="nav.research">Research</a>
						</li>
						<li>
							<a href="../resources.html" data-i18n="nav.resources">Resources</a>
						</li>
						<li><a href="../contact.html" data-i18n="nav.contact">Contact</a></li>
					</ul>
				</nav>
				<div class="language-toggle">
					<button id="lang-es" class="language-toggle-btn active">ES</button>
					<span class="language-toggle-separator">|</span>
					<button id="lang-en" class="language-toggle-btn">EN</button>
				</div>
				<a href="../contact.html" class="btn btn-primary" data-i18n="nav.joinUs"
					>Join Us</a
				>
			</div>
		</header>

		<section class="page-header">
			<div class="container">
				<div class="breadcrumb" style="margin-bottom: 1rem;">
					<a href="../mech-interp-course.html" class="btn btn-secondary" style="margin-right: 1rem;">← Back to Course</a>
				</div>
				<h1>Chapter 2: Transformers</h1>
				<p class="subtitle">
					Understanding the architecture and mechanics of transformer models
				</p>
			</div>
		</section>

		<section class="handbook-content">
			<div class="container">
				<div class="handbook-navigation">
					<div class="handbook-toc">
						<h3>Table of Contents</h3>
						<ul>
							<li><a href="#introduction">Introduction to Transformers</a></li>
							<li><a href="#architecture">Transformer Architecture</a></li>
							<li><a href="#attention">Attention Mechanisms</a></li>
							<li><a href="#language-models">Transformer-based Language Models</a></li>
							<li><a href="#conclusion">Conclusion</a></li>
						</ul>
					</div>
					<div class="chapter-navigation">
						<span class="prev-chapter disabled">Previous Chapter</span>
						<a href="chapter3.html" class="next-chapter">Next Chapter: Interpretability →</a>
					</div>
				</div>
				
				<div class="handbook-main">
					<h2 id="introduction">Introduction to Transformers</h2>
					<p>
						Transformers have revolutionized machine learning, particularly in natural language processing. This chapter provides a thorough introduction to the transformer architecture, including its key components and operating principles.
					</p>
					<p>
						The transformer model was introduced in the 2017 paper "Attention Is All You Need" by Vaswani et al. Unlike previous sequence-to-sequence models that relied on recurrence or convolution, transformers are based entirely on attention mechanisms, making them more parallelizable and efficient for training.
					</p>
					
					<h2 id="architecture">Transformer Architecture</h2>
					<p>
						The transformer architecture consists of an encoder and a decoder, each containing stacked layers of self-attention and feed-forward neural networks.
					</p>
					<h3>Encoder</h3>
					<p>
						The encoder processes the input sequence in parallel. Each encoder layer has two sub-layers:
					</p>
					<ul>
						<li>Multi-head self-attention mechanism</li>
						<li>Position-wise fully connected feed-forward network</li>
					</ul>
					<p>
						Each sub-layer employs residual connections and layer normalization.
					</p>
					<h3>Decoder</h3>
					<p>
						The decoder generates the output sequence, with each step predicting the next token. It contains three sub-layers:
					</p>
					<ul>
						<li>Masked multi-head self-attention mechanism (to prevent looking at future tokens)</li>
						<li>Multi-head attention over the encoder's output</li>
						<li>Position-wise fully connected feed-forward network</li>
					</ul>
					
					<h2 id="attention">Attention Mechanisms</h2>
					<p>
						Attention mechanisms allow the model to focus on different parts of the input when generating each part of the output. The self-attention mechanism in transformers, specifically the "Scaled Dot-Product Attention," computes attention weights as follows:
					</p>
					<pre>
Attention(Q, K, V) = softmax(QK^T/√d_k)V
					</pre>
					<p>
						Where Q (query), K (key), and V (value) are matrices, and d_k is the dimension of the keys.
					</p>
					<p>
						Multi-head attention allows the model to jointly attend to information from different representation subspaces, enabling it to capture different aspects of the input.
					</p>
					
					<h2 id="language-models">Transformer-based Language Models</h2>
					<p>
						Large language models like GPT (Generative Pre-trained Transformer) use transformer architectures with modifications. These models are trained on vast amounts of text data to predict the next token in a sequence.
					</p>
					<p>
						Key transformer-based language models include:
					</p>
					<ul>
						<li>BERT: Uses a bidirectional transformer for masked language modeling</li>
						<li>GPT series: Uses a decoder-only transformer for autoregressive language modeling</li>
						<li>T5: Uses the full encoder-decoder architecture for various text-to-text tasks</li>
					</ul>
					
					<h2 id="conclusion">Conclusion</h2>
					<p>
						Transformers have become the foundation of modern NLP and are expanding into other domains like computer vision and reinforcement learning. Understanding their architecture is essential for work in mechanistic interpretability, as it provides the necessary context for analyzing how these models process and represent information.
					</p>
					
					<div class="further-reading">
						<h3>Further Reading</h3>
						<ul>
							<li><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need (Vaswani et al., 2017)</a></li>
							<li><a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">A Mathematical Framework for Transformer Circuits</a></li>
							<li><a href="https://www.youtube.com/watch?v=wjZofJX0v4M" target="_blank">3Blue1Brown - But what is a GPT?</a></li>
						</ul>
					</div>
				</div>
			</div>
		</section>

		<footer class="site-footer">
			<div class="container">
				<div class="footer-content">
					<div class="footer-logo">
						<img src="../img/logo.svg" alt="BAISH Logo" class="footer-logo-img" />
						<p class="tagline" data-i18n="footer.tagline">
							Ensuring AI benefits humanity through research, education, and
							community
						</p>
					</div>
					<div class="footer-links">
						<ul>
							<li><a href="../about.html" data-i18n="nav.about">About</a></li>
							<li>
								<a href="../activities.html" data-i18n="nav.activities"
									>Activities</a
								>
							</li>
							<li>
								<a href="../research.html" data-i18n="nav.research">Research</a>
							</li>
							<li>
								<a href="../resources.html" data-i18n="nav.resources"
									>Resources</a
								>
							</li>
							<li><a href="../contact.html" data-i18n="nav.contact">Contact</a></li>
						</ul>
					</div>
				</div>
				<div class="footer-nav">
					<div class="footer-nav-left">
						<span data-i18n="footer.copyright"
							>&copy; 2024 BAISH - Buenos Aires AI Safety Hub</span
						>
						<a href="../privacy-policy.html" data-i18n="footer.privacy"
							>Privacy Policy</a
						>
					</div>
					<div class="footer-nav-right">
						<div class="social-links">
							<a
								href="https://twitter.com/baish_ai"
								target="_blank"
								aria-label="Twitter"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"
									/>
								</svg>
							</a>
							<a
								href="https://github.com/BAISH-AI"
								target="_blank"
								aria-label="GitHub"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"
									/>
								</svg>
							</a>
							<a
								href="https://www.linkedin.com/company/baish-ai"
								target="_blank"
								aria-label="LinkedIn"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"
									/>
									<rect x="2" y="9" width="4" height="12" />
									<circle cx="4" cy="4" r="2" />
								</svg>
							</a>
						</div>
					</div>
				</div>
			</div>
		</footer>

		<!-- JavaScript -->
		<script src="../js/translations.js"></script>
		<script src="../js/language-toggle.js"></script>
		<script src="../js/mobile-nav.js"></script>
		<script>
			// Force language application on page load
			document.addEventListener('DOMContentLoaded', function() {
				// Apply translations based on stored language
				const storedLang = localStorage.getItem('language') || 'es';
				currentLanguage = storedLang;
				updateLanguageToggleUI();
				applyTranslations();
			});
		</script>
	</body>
</html> 