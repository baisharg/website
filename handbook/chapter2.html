<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1.0"
		/>
		<title>Chapter 2: Transformers - BAISH Mech Interp Handbook</title>
		<meta
			name="description"
			content="Introduction to Transformers - BAISH Mechanistic Interpretability Handbook"
		/>
		<!-- Favicon -->
		<link rel="icon" href="../img/favicon.ico" />
		<!-- Google Fonts -->
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&family=Playfair+Display:wght@400;600&display=swap"
			rel="stylesheet"
		/>
		<!-- Custom CSS -->
		<link rel="stylesheet" href="../css/style.css" />
		<link rel="stylesheet" href="../css/language-toggle.css" />
	</head>
	<body>
		<a href="#main-content" class="skip-link">Skip to content</a>
		<header class="site-header">
			<div class="container">
				<div class="logo">
					<a href="../index.html">
						<img src="../img/logo.svg" alt="BAISH Logo" class="logo-img" />
						<span class="logo-text">BAISH - Buenos Aires AI Safety Hub</span>
					</a>
				</div>
				<button class="mobile-menu-button" aria-label="Toggle menu">
					<span></span>
					<span></span>
					<span></span>
				</button>
				<nav class="main-nav">
					<ul>
						<li><a href="../about.html" data-i18n="nav.about">About</a></li>
						<li>
							<a href="../activities.html" data-i18n="nav.activities"
								>Activities</a
							>
						</li>
						<li>
							<a href="../research.html" data-i18n="nav.research">Research</a>
						</li>
						<li>
							<a href="../resources.html" data-i18n="nav.resources"
								>Resources</a
							>
						</li>
						<li>
							<a href="../contact.html" data-i18n="nav.contact">Contact</a>
						</li>
					</ul>
				</nav>
				<div class="language-toggle">
					<button id="lang-es" class="language-toggle-btn active">ES</button>
					<span class="language-toggle-separator">|</span>
					<button id="lang-en" class="language-toggle-btn">EN</button>
				</div>
				<a href="../contact.html" class="btn btn-primary" data-i18n="nav.joinUs"
					>Join Us</a
				>
			</div>
		</header>

		<section id="main-content" class="page-header">
			<div class="container">
				<div class="breadcrumb" style="margin-bottom: 1rem">
					<a
						href="../mech-interp-course.html"
						class="btn btn-secondary"
						style="margin-right: 1rem"
						>← Back to Course</a
					>
				</div>
				<h1>Chapter 2: Transformers</h1>
				<p class="subtitle">
					Understanding the architecture and mechanics of transformer models
				</p>
			</div>
		</section>

		<section class="handbook-content">
			<div class="container">
				<div class="handbook-navigation">
					<div class="handbook-toc">
						<h3>Table of Contents</h3>
						<ul>
							<li><a href="#introduction">Introduction to Transformers</a></li>
							<li><a href="#architecture">Transformer Architecture</a></li>
							<li><a href="#attention">Attention Mechanisms</a></li>
							<li>
								<a href="#language-models">Transformer-based Language Models</a>
							</li>
							<li><a href="#conclusion">Conclusion</a></li>
						</ul>
					</div>
					<div class="chapter-navigation">
						<span class="prev-chapter disabled">Previous Chapter</span>
						<a href="chapter3.html" class="next-chapter"
							>Next Chapter: Interpretability →</a
						>
					</div>
				</div>

				<div class="handbook-main">
					<h2 id="introduction">Introduction to Transformers</h2>
					<p>
						Transformers have revolutionized machine learning, particularly in
						natural language processing. This chapter provides a thorough
						introduction to the transformer architecture, including its key
						components and operating principles.
					</p>
					<p>
						The transformer model was introduced in the 2017 paper "Attention Is
						All You Need" by Vaswani et al. Unlike previous sequence-to-sequence
						models that relied on recurrence or convolution, transformers are
						based entirely on attention mechanisms, making them more
						parallelizable and efficient for training.
					</p>

					<h2 id="architecture">Transformer Architecture</h2>
					<p>
						The transformer architecture consists of an encoder and a decoder,
						each containing stacked layers of self-attention and feed-forward
						neural networks.
					</p>
					<h3>Encoder</h3>
					<p>
						The encoder processes the input sequence in parallel. Each encoder
						layer has two sub-layers:
					</p>
					<ul>
						<li>Multi-head self-attention mechanism</li>
						<li>Position-wise fully connected feed-forward network</li>
					</ul>
					<p>
						Each sub-layer employs residual connections and layer normalization.
					</p>
					<h3>Decoder</h3>
					<p>
						The decoder generates the output sequence, with each step predicting
						the next token. It contains three sub-layers:
					</p>
					<ul>
						<li>
							Masked multi-head self-attention mechanism (to prevent looking at
							future tokens)
						</li>
						<li>Multi-head attention over the encoder's output</li>
						<li>Position-wise fully connected feed-forward network</li>
					</ul>

					<h2 id="attention">Attention Mechanisms</h2>
					<p>
						Attention mechanisms allow the model to focus on different parts of
						the input when generating each part of the output. The
						self-attention mechanism in transformers, specifically the "Scaled
						Dot-Product Attention," computes attention weights as follows:
					</p>
					<pre>
Attention(Q, K, V) = softmax(QK^T/√d_k)V
					</pre
					>
					<p>
						Where Q (query), K (key), and V (value) are matrices, and d_k is the
						dimension of the keys.
					</p>
					<p>
						Multi-head attention allows the model to jointly attend to
						information from different representation subspaces, enabling it to
						capture different aspects of the input.
					</p>

					<h2 id="language-models">Transformer-based Language Models</h2>
					<p>
						Large language models like GPT (Generative Pre-trained Transformer)
						use transformer architectures with modifications. These models are
						trained on vast amounts of text data to predict the next token in a
						sequence.
					</p>
					<p>Key transformer-based language models include:</p>
					<ul>
						<li>
							BERT: Uses a bidirectional transformer for masked language
							modeling
						</li>
						<li>
							GPT series: Uses a decoder-only transformer for autoregressive
							language modeling
						</li>
						<li>
							T5: Uses the full encoder-decoder architecture for various
							text-to-text tasks
						</li>
					</ul>

					<h2 id="conclusion">Conclusion</h2>
					<p>
						Transformers have become the foundation of modern NLP and are
						expanding into other domains like computer vision and reinforcement
						learning. Understanding their architecture is essential for work in
						mechanistic interpretability, as it provides the necessary context
						for analyzing how these models process and represent information.
					</p>

					<div class="further-reading">
						<h3>Further Reading</h3>
						<ul>
							<li>
								<a href="https://arxiv.org/abs/1706.03762" target="_blank"
									>Attention Is All You Need (Vaswani et al., 2017)</a
								>
							</li>
							<li>
								<a
									href="https://transformer-circuits.pub/2021/framework/index.html"
									target="_blank"
									>A Mathematical Framework for Transformer Circuits</a
								>
							</li>
							<li>
								<a
									href="https://www.youtube.com/watch?v=wjZofJX0v4M"
									target="_blank"
									>3Blue1Brown - But what is a GPT?</a
								>
							</li>
						</ul>
					</div>
				</div>
			</div>
		</section>

		<footer class="site-footer">
			<div class="container">
				<div class="footer-content">
					<div class="footer-logo">
						<img
							src="../img/logo.svg"
							alt="BAISH Logo"
							class="footer-logo-img"
						/>
						<p class="tagline" data-i18n="footer.tagline">
							Ensuring AI benefits humanity through research, education, and
							community
						</p>
					</div>
					<div class="footer-links">
						<ul>
							<li><a href="../about.html" data-i18n="nav.about">About</a></li>
							<li>
								<a href="../activities.html" data-i18n="nav.activities"
									>Activities</a
								>
							</li>
							<li>
								<a href="../research.html" data-i18n="nav.research">Research</a>
							</li>
							<li>
								<a href="../resources.html" data-i18n="nav.resources"
									>Resources</a
								>
							</li>
							<li>
								<a href="../contact.html" data-i18n="nav.contact">Contact</a>
							</li>
						</ul>
					</div>
				</div>
				<div class="footer-nav">
					<div class="footer-nav-left">
						<span data-i18n="footer.copyright"
							>&copy; 2024 BAISH - Buenos Aires AI Safety Hub</span
						>
						<a href="../privacy-policy.html" data-i18n="footer.privacy"
							>Privacy Policy</a
						>
					</div>
					<div class="footer-nav-right">
						<div class="social-links">
							<a
								href="https://twitter.com/baish_ai"
								target="_blank"
								aria-label="Twitter"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"
									/>
								</svg>
							</a>
							<a
								href="https://github.com/BAISH-AI"
								target="_blank"
								aria-label="GitHub"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"
									/>
								</svg>
							</a>
							<a
								href="https://www.linkedin.com/company/baish-ai"
								target="_blank"
								aria-label="LinkedIn"
							>
								<svg
									xmlns="http://www.w3.org/2000/svg"
									width="20"
									height="20"
									viewBox="0 0 24 24"
									fill="none"
									stroke="currentColor"
									stroke-width="2"
									stroke-linecap="round"
									stroke-linejoin="round"
								>
									<path
										d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"
									/>
									<rect x="2" y="9" width="4" height="12" />
									<circle cx="4" cy="4" r="2" />
								</svg>
							</a>
						</div>
					</div>
				</div>
			</div>
		</footer>

		<!-- JavaScript -->
		<script src="../js/translations.js"></script>
		<script src="../js/language-toggle.js"></script>
		<script src="../js/mobile-nav.js"></script>
		<script>
			// Force language application on page load
			document.addEventListener("DOMContentLoaded", function () {
				// Apply translations based on stored language
				const storedLang = localStorage.getItem("language") || "es";
				currentLanguage = storedLang;
				updateLanguageToggleUI();
				applyTranslations();
			});
		</script>
	</body>
</html>
