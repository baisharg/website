// Translations for the entire website
const translations = {
  en: {
    // Navigation
    nav: {
      home: 'Home',
      about: 'About',
      activities: 'Activities',
      research: 'Research',
      resources: 'Resources',
      mechInterpCourse: 'Mech Interp Course',
      contact: 'Contact',
      joinUs: 'Join Us'
    },

    // Footer
    footer: {
      privacyPolicy: 'Privacy Policy',
      copyright: '¬© 2025 BAISH - Buenos Aires AI Safety Hub',
      tagline: 'Buenos Aires AI Safety Hub'
    },

    // Privacy Policy page
    privacy: {
      pageHeader: {
        breadcrumb: 'Privacy Policy',
        title: 'Privacy Policy'
      },
      intro: {
        title: 'Our Approach to Privacy',
        content:
          'At BAISH (Buenos Aires AI Safety Hub), we are committed to respecting your privacy. This Privacy Policy outlines our practices regarding the collection, use, and protection of your information when you use our website and services.'
      },
      dataCollection: {
        title: 'Data Collection',
        content:
          'We collect minimal personal information. The only personal data we collect is email addresses when users voluntarily sign up for our newsletter through Substack. This information is stored and managed by Substack according to their privacy policy.'
      },
      noTracking: {
        title: 'No Tracking or Cookies',
        content:
          'We do not use cookies, analytics, tracking tools, or any other technology to collect data about you. We do not monitor your browsing activities or gather information about your online behaviors.'
      },
      thirdParty: {
        title: 'Third-Party Services',
        content:
          "Our newsletter is managed through Substack. When you subscribe to our newsletter, your email address is shared with and stored by Substack. Please refer to Substack's Privacy Policy to understand how they handle your information."
      },
      yourRights: {
        title: 'Your Rights',
        content:
          'You have the right to unsubscribe from our newsletter at any time by clicking the unsubscribe link in any of our emails or by contacting us directly. If you have any questions about your data or wish to access, correct, or delete your information, please contact us.'
      },
      changes: {
        title: 'Changes to This Policy',
        content:
          'We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page.'
      },
      contact: {
        title: 'Contact Us',
        content:
          'If you have any questions about this Privacy Policy, please contact us through our Contact page.'
      }
    },

    // Home page
    home: {
      hero: {
        title: 'Ensuring AI Benefits Humanity',
        subtitle:
          "BAISH's student-led initiative exploring safe and beneficial AI development",
        paragraph1_part1:
          'As artificial intelligence models advance in capabilities',
        paragraph1_part2:
          ', we expect them to have an increasingly profound impact on our society',
        paragraph1_part3:
          '. It is essential that this impact is positive, and that the decisions made by these systems are transparent, reliable, and accountable',
        paragraph1_part4: ' to the people affected by them.',
        paragraph2_part1:
          'We believe that reducing the risks associated with advanced AI models',
        paragraph2_part2:
          ' is one of the most important challenges of our time. We also believe it is an open and exciting problem',
        paragraph2_part3:
          ', with ample opportunities for more researchers to advance in this field',
        paragraph2_part4: '.',
        paragraph3:
          "BAISH's mission is to support students in entering this field and conducting research on this topic.",
        cta: 'Get Involved'
      },
      intro: {
        title: 'Why AI Safety Matters',
        paragraph1:
          'Advanced AI systems are becoming increasingly capable, with the potential to transform society in profound ways. The field of AI Safety focuses on ensuring these systems are aligned with human values and beneficial to humanity.',
        paragraph2:
          'At the BAISH - Buenos Aires AI Safety Hub, we bring together students from Computer Science, Physics, Mathematics and other enthusiasts to understand and address these challenges through research, education, and community.',
        point1: 'Understanding AI capabilities and risks',
        point2: 'Developing alignment techniques',
        point3: 'Conducting original research',
        point4: 'Building a community of researchers',
        learnMore: 'Learn more about AI Safety ‚Üí'
      },
      upcomingEvent: {
        title: 'Weekly Discussion Group: Interpretability Methods',
        eventType: 'Upcoming Event',
        time: '18:00 - 19:30',
        location: 'Pabellon 0+inf, Room 1604, Ciudad Universitaria',
        description:
          'Join us for an interactive discussion on recent advances in neural network interpretability methods and their implications for AI alignment.',
        rsvp: 'RSVP Now'
      },
      activities: {
        title: 'Our Activities',
        card1: {
          title: 'AGI Safety Fundamentals Cohort',
          description:
            'An 8-week guided course covering the essential concepts in AI alignment and safety.',
          status: 'Starting in the second semester',
          link: 'Learn More ‚Üí'
        },
        card2: {
          title: 'Weekly Discussion Group',
          description:
            'Weekly meetings to discuss recent papers, concepts, and developments in AI safety.',
          status: 'Every Tuesday @ 3pm',
          link: 'Learn More ‚Üí'
        },
        card3: {
          title: 'Paper Reading Club',
          description: 'Student led presentations of AI Safety papers.',
          status: 'Every Thursday @ 2pm',
          link: 'Learn More ‚Üí'
        },
        card4: {
          title: 'Mech Interp Course',
          description:
            'Intensive 1-month course on mechanistic interpretability methods.',
          status: 'Starts end of April 2025',
          link: 'Learn More ‚Üí'
        },
        discussion: {
          title: 'Weekly Discussion Group',
          description:
            'Join our weekly discussion group on AI safety topics and recent field advancements.',
          addToCalendar: 'Add to Calendar'
        }
      },
      getInvolved: {
        title: 'Get Involved',
        mailingList: {
          title: 'Join our Mailing List',
          description:
            'Stay updated with our activities, events, and opportunities.'
        },
        telegram: {
          title: 'Join our Telegram Community',
          description: 'Connect with fellow students interested in AI safety.',
          button: 'Join Telegram Group'
        }
      },
      events: {
        // Added new events section
        title: 'Upcoming Events',
        description:
          'Join our upcoming events, discussions, and workshops on AI safety. All events are open to the community.',
        subscribeCalendar: 'Subscribe to Calendar'
      }
    },

    // Hackathon translations
    hackathon: {
      title: 'AI Safety Hackathon: Model Routing',
      subtitle: 'Beyond Single Models - Revoluci√≥n del Model Routing',
      prize: '$2,000 USD en Premios',
      days: 'D√≠as',
      hours: 'Horas',
      minutes: 'Min',
      seconds: 'Seg',
      register: '¬°Inscribite ahora!',
      urgency: '¬°Cupos limitados - No te quedas afuera!',
      eventTitle: 'AI Safety Hackathon - Model Routing',
      eventDate: '30 de Mayo - 1 de Junio, 2025',
      venue: '@ Aleph Hub',
      foodIncluded: 'ü•ó Comida vegana incluida',
      eventDescription:
        "Join Argentina's first AI Safety hackathon. We'll explore revolutionary Model Routing techniques that go beyond single models to create safer and more efficient AI systems. Compete for $2,000 USD in prizes!",
      registerEvent: '¬°Reg√≠strate ahora!',
      moreInfo: 'M√°s Informaci√≥n & Registro ‚Üí',
      tracks: {
        title: 'Tracks de Desaf√≠o',
        track1: {
          title: 'Desarrollo de Modelo de Evaluaci√≥n',
          description:
            'Construye modelos de evaluaci√≥n especializados que eval√∫an capacidades de IA en dimensiones relevantes para la seguridad.'
        },
        track2: {
          title: 'Sistemas de Enrutamiento Inteligente',
          description:
            'Desarrolla sistemas que dirigen consultas de manera inteligente a modelos especializados apropiados.'
        },
        track3: {
          title: 'Descomposici√≥n de Tareas',
          description:
            'Crea marcos de trabajo que descomponen solicitudes complejas en pasos manejables para diferentes modelos.'
        },
        track4: {
          title: 'Integraci√≥n de Modelos',
          description:
            'Construye marcos de trabajo para una integraci√≥n f√°cil de modelos especializados en la arquitectura de enrutamiento.'
        }
      },
      feature1: {
        title: 'Premios Atractivos',
        description: '$2,000 USD distribuidos entre los mejores proyectos'
      },
      feature2: {
        title: 'Model Routing',
        description: 'T√©cnicas innovadoras para sistemas multi-modelo'
      },
      feature3: {
        title: 'Support',
        description: 'Weekend support and mentoring'
      }
    },

    // About page
    about: {
      pageHeader: {
        breadcrumb: 'Home / About AI Safety',
        title: 'Understanding AI Safety'
      },
      coreConcepts: {
        whatIs: {
          title: 'What is AI Safety?',
          content:
            'AI Safety is a research field focused on ensuring that advanced artificial intelligence systems remain beneficial, aligned with human values, and under human control as they become more capable. It encompasses technical research areas like alignment, interpretability, and robustness, as well as governance considerations about how AI systems should be developed and deployed.'
        },
        whyMatters: {
          title: 'Why It Matters',
          content:
            "As AI systems become more powerful and autonomous, they may develop capabilities that could lead to unintended consequences if not properly designed and controlled. The stakes are high: advanced AI could help solve humanity's greatest challenges, but also poses significant risks if developed without adequate safety measures. The field aims to maximize the benefits while minimizing potential harms."
        },
        challenges: {
          title: 'Key Risks & Challenges',
          alignment: {
            title: 'Alignment Problem',
            content:
              'Ensuring AI systems pursue goals aligned with human values and intentions, even as they become more capable.'
          },
          interpretability: {
            title: 'Interpretability',
            content:
              'Developing techniques to understand how AI systems make decisions and represent knowledge.'
          },
          robustness: {
            title: 'Robustness',
            content:
              'Creating systems that behave safely even when deployed in new environments or facing unexpected situations.'
          },
          powerSeeking: {
            title: 'Power-seeking Behavior',
            content:
              'Preventing AI systems from developing instrumental goals that conflict with human welfare.'
          },
          coordination: {
            title: 'Coordination Challenges',
            content:
              'Ensuring that safety standards are maintained across all major AI development efforts globally.'
          }
        }
      },
      externalResources: {
        title: 'Learn More About AI Safety',
        cards: {
          alignmentForum: {
            title: 'Alignment Forum',
            description:
              'A forum dedicated to technical research in AI alignment, with papers and discussions from leading researchers.',
            difficulty: 'Technical',
            cta: 'Visit'
          },
          lessWrong: {
            title: 'LessWrong',
            description:
              'A community blog focused on human rationality and the implications of artificial intelligence.',
            difficulty: 'Intermediate',
            cta: 'Visit'
          },
          eightyThousandHours: {
            title: '80,000 Hours',
            description:
              "Career guidance for working on the world's most pressing problems, including AI safety.",
            difficulty: 'Introductory',
            cta: 'Visit'
          },
          stampy: {
            title: "Stampy's Wiki",
            description:
              'A collaborative wiki providing accessible explanations of AI alignment concepts.',
            difficulty: 'Introductory',
            cta: 'Visit'
          }
        }
      },
      ourApproach: {
        title: 'Our Approach',
        focusAreas: {
          title: 'Focus Areas',
          intro:
            'At BAISH - Buenos Aires AI Safety Hub, we focus on several key areas within AI safety research:',
          areas: [
            'Mechanistic interpretability of neural networks',
            'Alignment techniques for large language models',
            'Robust training methodologies',
            'Value learning and preference inference'
          ]
        },
        contributions: {
          title: 'Our Contribution',
          intro: 'We contribute to the field through:',
          items: [
            'Supporting student research projects',
            'Developing educational resources in Spanish',
            'Building a regional community of AI safety researchers',
            'Organizing workshops and training programs',
            'Mentoring students interested in AI safety careers'
          ]
        }
      },
      team: {
        title: 'Core Team',
        members: {
          eitan: {
            name: 'Eitan Sprejer',
            title: 'Co-founding Director',
            bio: 'Physics student with a passion for AI safety and alignment.'
          },
          luca: {
            name: 'Luca De Leo',
            title: 'Co-founding Director',
            bio: 'Operations Generalist with experience working in AI Safety Organizations'
          },
          lucas: {
            name: 'Lucas Vitali',
            title: 'Communications Director',
            bio: 'Computer Science student with a passion for science communications'
          },
          sergio: {
            name: 'Sergio Abriola, PhD',
            title: 'Advisor',
            bio: 'Mathematics PhD and professor with a passion for AI Safety'
          }
        }
      }
    },

    // Activities page
    activities: {
      pageHeader: {
        breadcrumb: 'Home / Activities',
        title: 'Our Activities',
        subtitle:
          'Join our community and participate in AI safety research and learning'
      },
      agiFundamentals: {
        title: 'AGI Safety Fundamentals Cohort',
        status: 'Comienza en el segundo semestre',
        description:
          'The AGI Safety Fundamentals cohort is an 8-week guided course covering the essential concepts in AI alignment and safety. Participants read curated materials and meet weekly to discuss the readings with a facilitator.',
        description2:
          'This program is based on the BlueDot AGI Safety Fundamentals curriculum and provides a structured introduction to the field of AI safety.',
        whatToExpect: {
          title: 'What to Expect',
          items: [
            '2-hour weekly discussion sessions',
            '1-3 hours of reading per week',
            'Small groups of 6-12 participants',
            'Experienced facilitators to guide discussions',
            'Certificate of completion'
          ]
        },
        details: {
          title: 'Program Details',
          duration: 'Duration:',
          durationValue: '10-12 weeks',
          period: 'Fellowship Period:',
          periodValue: 'August - December 2025'
        },
        viewCurriculumButton: 'View Curriculum'
      },
      weeklyDiscussion: {
        title: 'Weekly Discussion Group',
        status: 'Todos los martes a las 14:30',
        description:
          'Our Weekly Discussion Group provides a casual forum to discuss recent papers, concepts, and developments in AI safety. These sessions are open to anyone interested in the field, regardless of prior knowledge.',
        description2:
          'Each week features a different topic, announced in advance via our mailing list and Telegram group.',
        format: {
          title: 'Format',
          items: [
            '90-minute discussions led by a rotating facilitator',
            "Brief presentation on the week's topic (15-20 minutes)",
            'Open discussion and Q&A',
            'Optional pre-reading materials shared in advance'
          ]
        },
        participation: {
          title: 'Participation',
          description:
            "No registration required. Just show up! If it's your first time attending, we recommend arriving 10 minutes early to meet the organizers."
        },
        nextDiscussion: {
          title: 'Next Discussion',
          date: 'Date:',
          dateValue: 'March 25, 2025',
          time: 'Time:',
          timeValue: '18:00 - 19:30',
          location: 'Location:',
          locationValue: 'Pabell√≥n 0+inf, Room 1604, Ciudad Universitaria',
          topic: 'Topic:',
          topicValue: 'Interpretability Methods',
          facilitator: 'Facilitator:',
          facilitatorValue: 'Eitan Sprejer',
          button: 'Join Telegram for Updates'
        }
      },
      paperReading: {
        title: 'Paper Reading Club',
        status: 'Todos los martes a las 14:00',
        description:
          'The Paper Reading Club conducts deep dives into foundational and recent papers in AI safety research. Unlike the more casual discussion group, this activity involves a close examination of specific research papers.',
        description2:
          'Participants are expected to read the selected paper in advance and come prepared to discuss its methods, results, and implications.',
        selectionCriteria: {
          title: 'Paper Selection Criteria',
          items: [
            'Importance to the field of AI safety',
            'Technical relevance to current research directions',
            'Mix of foundational papers and recent publications',
            'Accessibility to graduate and advanced undergraduate students'
          ]
        },
        sessionFormat: {
          title: 'Session Format',
          items: [
            'Brief overview of the paper (5-10 minutes)',
            'Section-by-section discussion',
            'Examination of methods and results',
            'Critical evaluation of claims and limitations',
            'Discussion of potential follow-up research'
          ]
        },
        nextSession: {
          title: 'Next Paper Session',
          date: 'Date:',
          dateValue: 'March 21, 2025',
          time: 'Time:',
          timeValue: '17:00 - 18:30',
          location: 'Location:',
          locationValue: 'Pabell√≥n 0+inf, Room 1604, Ciudad Universitaria',
          paper: 'Paper:',
          paperValue: '"Mechanistic Interpretability for Language Models"',
          lead: 'Discussion Lead:',
          leadValue: 'Eitan Sprejer',
          button: 'Access Reading List'
        }
      },
      mechInterp: {
        title: 'Mechanistic Interpretability Course',
        status: 'Starts end of April 2025',
        description:
          'The Mechanistic Interpretability Course is an intensive 1-month program focused on techniques for understanding the internal mechanisms of neural networks. This course combines theoretical learning with practical projects.',
        description2:
          'Mechanistic interpretability is a key area of AI safety research, seeking to make AI systems more transparent and understandable.',
        curriculum: {
          title: 'Curriculum Overview',
          items: [
            'Fundamentals of neural network architectures',
            'Feature visualization techniques',
            'Circuit analysis in transformer models',
            'Techniques for analyzing attention mechanisms',
            'Gradient-based attribution methods',
            'Final project: Interpreting a specific model component'
          ]
        },
        timeCommitment: {
          title: 'Time Commitment',
          items: [
            '2 classes per week (2 hours each)',
            '1 practical session per week (3 hours)',
            'Individual project work (5-10 hours per week)',
            'Final project presentation'
          ]
        },
        prerequisites: {
          title: 'Prerequisites',
          items: [
            'Strong programming skills (Python)',
            'Experience with deep learning frameworks (preferably PyTorch)',
            'Familiarity with basic neural network architectures',
            'Linear algebra and calculus'
          ]
        },
        courseDetails: {
          title: 'Course Details',
          duration: 'Duration:',
          durationValue: '4 weeks',
          startDate: 'Start Date:',
          startDateValue: 'June 2, 2025',
          endDate: 'End Date:',
          endDateValue: 'June 27, 2025',
          deadline: 'Application Deadline:',
          deadlineValue: 'May 15, 2025',
          location: 'Location:',
          locationValue: 'Hybrid (In-person and Zoom)',
          instructors: 'Instructors:',
          instructorsValue: 'Dr. Laura Fernandez, Carlos Mendez',
          button: 'Express Interest'
        },
        viewCourseButton: 'View Detailed Course Information'
      },
      calendar: {
        title: 'Upcoming Events Calendar',
        subscribe: 'Subscribe to Google Calendar for All Events',
        subscribeLuma: 'Subscribe to Events Calendar' // Added Luma button text
      }
    },

    // Resources page
    resources: {
      pageHeader: {
        breadcrumb: 'Home / Resources',
        title: 'Learning Resources',
        subtitle:
          'Curated materials for exploring AI safety concepts and our newsletter archive'
      },
      learningPathways: {
        title: 'Learning Pathways',
        intro:
          "We've organized resources into three learning paths depending on your background and experience level.",
        pathways: {
          beginners: {
            title: 'Beginners',
            difficulty: 'Accessible to All',
            description:
              'For those new to AI safety concepts who want to understand the key ideas and concerns.',
            resources: {
              resource1: {
                title: 'Future of Life Institute: AI Risk Introduction',
                type: 'Overview'
              },
              resource2: {
                title: 'Cold Takes: Transformative AI series',
                type: 'Blog Series'
              },
              resource3: {
                title: 'Rob Miles: AI Safety Intro',
                type: 'Video'
              },
              resource4: {
                title: '80,000 Hours: AI Risk Problem Profile',
                type: 'Overview'
              },
              resource5: {
                title: "Stampy's AI Safety Wiki",
                type: 'Wiki'
              }
            },
            button: 'Get Beginner Recommendations'
          },
          intermediate: {
            title: 'Intermediate',
            difficulty: 'Some Technical Background',
            description:
              'For those with some understanding of machine learning who want to explore core AI safety challenges.',
            resources: {
              resource1: {
                title: 'AGI Safety Fundamentals Curriculum',
                type: 'Course Materials'
              },
              resource2: {
                title: 'Technical Alignment Fundamentals',
                type: 'Reading List'
              },
              resource3: {
                title: 'Anthropic: Core Views on AI Safety',
                type: 'Company Perspective'
              },
              resource4: {
                title: 'Training Language Models to Follow Instructions',
                type: 'Research Paper'
              },
              resource5: {
                title: 'DeepLearning.AI: Alignment Techniques for LLMs',
                type: 'Course'
              }
            },
            button: 'Get Intermediate Recommendations'
          },
          advanced: {
            title: 'Advanced',
            difficulty: 'Strong Technical Background',
            description:
              'For those with strong ML/AI knowledge who want to engage with cutting-edge research.',
            resources: {
              resource1: {
                title: 'Distill: Circuits Thread',
                type: 'Interpretability Research'
              },
              resource2: {
                title: 'Anthropic: Transformer Circuits',
                type: 'Research Series'
              },
              resource3: {
                title: 'Discovering Latent Knowledge in Language Models',
                type: 'Research Paper'
              },
              resource4: {
                title: 'Language Agents for Alignment Research',
                type: 'Research Paper'
              },
              resource5: {
                title: 'Alignment Forum: Advanced Concepts',
                type: 'Technical Discussion'
              }
            },
            button: 'Get Advanced Recommendations'
          }
        }
      },
      recommendedBooks: {
        title: 'Recommended Books',
        books: {
          book1: {
            title: 'Uncontrollable',
            author: 'Darren McKee (2023)',
            description:
              'Uncontrollable uses engaging analogies and relatable examples to summarize AI for beginners, and unpacks AI risk and safety for readers without a technical background.'
          },
          book2: {
            title: 'Human Compatible',
            author: 'Stuart Russell (2019)',
            description:
              "A leading AI researcher's compelling case for how to ensure that artificial intelligence remains beneficial to humanity."
          },
          book3: {
            title: 'Superintelligence',
            author: 'Nick Bostrom (2014)',
            description:
              'Explores the potential risks and paths to superintelligent AI, addressing key questions about the future of intelligence.'
          },
          book4: {
            title: 'The Alignment Problem',
            author: 'Brian Christian (2020)',
            description:
              'An exploration of the growing field of AI alignment, explaining technical concepts in an accessible way.'
          },
          book5: {
            title: 'Life 3.0',
            author: 'Max Tegmark (2017)',
            description:
              'Examines how artificial intelligence might affect life in the future and what choices we face in shaping that future.'
          }
        }
      }
    },

    // Contact page
    contact: {
      pageHeader: {
        breadcrumb: 'Home / Contact',
        title: 'Contact Us',
        subtitle: 'Get in touch with our team and join our community'
      },
      contactInfo: {
        email: {
          title: 'Email',
          description: 'For general inquiries, suggestions, or questions:'
        },
        telegram: {
          title: 'Telegram',
          description:
            'Sumate a nuestro canal comunitario para discusiones y actualizaciones:'
        },
        location: {
          title: 'Ubicaci√≥n',
          description:
            'Estamos ubicados en el Departamento de Ciencias de la Computaci√≥n:'
        },
        socialMedia: {
          title: 'Redes Sociales',
          description: 'Seguinos para actualizaciones y anuncios:'
        }
      },
      getInvolved: {
        title: 'Get Involved',
        intro:
          'There are multiple ways to participate in our community and activities.',
        mailingList: {
          title: 'Join our Mailing List',
          description:
            'Stay updated with our events, activities, and opportunities by subscribing to our mailing list. We send monthly newsletters and important announcements.'
        }
      },
      form: {
        name: 'What is your name?',
        email: 'What is your email?',
        message: 'Message',
        clear: 'Clear form',
        submit: 'Submit'
      },
      faq: {
        title: 'Frequently Asked Questions',
        questions: {
          q1: {
            question: 'Do I need to be a UBA student to participate?',
            answer:
              'Most of our activities are primarily designed for UBA students, but we welcome participants from other universities for our discussion groups and paper reading sessions. Research fellowships are currently limited to UBA students.'
          },
          q2: {
            question: 'What background do I need to participate?',
            answer:
              'This varies by activity. Our discussion groups are open to anyone with an interest in AI safety, regardless of technical background. For more technical activities like the Mechanistic Interpretability course or the Research Fellowship, some background in computer science, mathematics, or AI/ML is expected.'
          },
          q3: {
            question: 'Are your activities conducted in English or Spanish?',
            answer:
              'We conduct most of our activities in both languages. Discussion groups are typically in Spanish, while some technical sessions may be in English, especially when using materials from international sources. Our written materials are available in both languages whenever possible.'
          },
          q4: {
            question:
              "How can I start learning about AI safety if I'm a complete beginner?",
            answer:
              'We recommend starting with our beginner resources on the Resources page and joining our weekly discussion group. The discussion group is a great place to ask questions and learn from others in an informal setting.'
          },
          q5: {
            question: 'Can I propose a new activity or research direction?',
            answer:
              "Absolutely! We're always open to new ideas. Contact us at aisafetyarg@gmail.com with your proposal, and one of our coordinators will discuss it with you."
          }
        }
      }
    },

    // Research page
    research: {
      pageHeader: {
        breadcrumb: 'Research',
        title: 'Our Research',
        subtitle: 'Student-led projects exploring critical AI safety challenges'
      },
      overview: {
        approach: {
          title: 'Research Approach',
          paragraph1:
            'At BAISH - Buenos Aires AI Safety Hub, we support student-led research into core AI safety challenges. Our projects range from technical work on interpretability and alignment to more conceptual explorations of AI governance and ethics.',
          paragraph2:
            'Research projects are typically conducted through our Research Fellowship program or as collaborations between students and faculty members. We encourage rigorous methods, creative thinking, and interdisciplinary approaches.'
        },
        focusAreas: {
          title: 'Focus Areas',
          interpretability:
            '<strong>Interpretabilidad y Transparencia</strong> - Understanding how neural networks represent and process information',
          alignment:
            '<strong>Alignment Techniques</strong> - Developing methods to align AI systems with human values and intentions',
          robustness:
            '<strong>Robustez</strong> - Creating AI systems that maintain safe behavior in new environments',
          valueLearning:
            '<strong>Aprendizaje de Valores</strong> - Inferir human preferences from feedback and demonstration'
        }
      },
      projects: {
        title: 'Research Projects',
        filterBy: 'Filter by:',
        filters: {
          all: 'All',
          interpretability: 'Interpretabilidad',
          alignment: 'Alineaci√≥n',
          robustness: 'Robustez',
          valueLearning: 'Aprendizaje de Valores'
        }
      },
      publications: {
        title: 'Publications',
        intro: 'Selected publications by our members in peer-reviewed venues.'
      },
      ongoingResearch: {
        title: 'Ongoing Research',
        intro:
          'Current projects under development by our research fellows and collaborators.'
      },
      joinResearch: {
        title: 'Interested in Conducting Research with Us?',
        description:
          "We welcome students from Computer Science, Mathematics, Physics, and related fields to join our research efforts. Whether you're an undergraduate looking for research experience or a graduate student interested in AI safety, we have opportunities for you.",
        buttons: {
          fellowships: 'Learn About Research Fellowships',
          contact: 'Contact Research Coordinators'
        }
      }
    },

    // Common elements
    common: {
      learnMore: 'Learn More',
      readMore: 'Read More',
      upcoming: 'Upcoming',
      active: 'Active',
      completed: 'Completed'
    },

    // Mech Interp Course page
    mechInterpCourse: {
      pageHeader: {
        title: 'Mechanistic Interpretability Course',
        subtitle:
          'A comprehensive introduction to Mechanistic Interpretability for Large Language Models'
      },
      backToHome: '‚Üê Back to Home',
      intro: {
        paragraph1:
          'Este es un curso r√°pido de 4 sesiones dise√±ado para introducir el campo de la Interpretabilidad Mecan√≠stica para Modelos de Lenguaje Grandes (LLMs). El curso incluye materiales te√≥ricos, introduce librer√≠as de Python para interpretabilidad, discute art√≠culos recientes en el campo (publicados por organizaciones como Anthropic y Google DeepMind), y proporciona ejercicios pr√°cticos.',
        paragraph2:
          'El campo de la Interpretabilidad Mecan√≠stica ha ganado r√°pidamente popularidad en los √∫ltimos a√±os, con m√°s de 90 papers aceptados en ICML 2024. Su objetivo principal es entender la l√≥gica detr√°s de las decisiones de los modelos de aprendizaje autom√°tico. Este conocimiento puede aplicarse para mejorar la transparencia y confianza en modelos existentes, as√≠ como para entender mejor c√≥mo estos modelos aprenden.',
        paragraph3:
          'Poco despu√©s de la finalizaci√≥n de este curso, BAISH organizar√° un Hackath√≥n de Interpretabilidad Mecan√≠stica en FGV ‚Äî ¬°detalles por anunciar! Recomendamos encarecidamente que cualquier persona interesada en participar en el Hackath√≥n complete este curso como introducci√≥n al tema, lo que tambi√©n aumentar√° sus posibilidades de recibir un premio en la competencia.'
      },
      schedule: {
        title: 'Cronograma del Curso',
        session1: {
          title: 'Sesi√≥n 1 - Transformers e Interpretabilidad',
          date: '23 de agosto, viernes, a partir de las 14:30',
          location: 'Auditorio 537',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulos 2 y 3 del Manual',
          part1: '45 min: Introducci√≥n a transformers y atenci√≥n',
          part2: '20 min: Pausa para caf√©',
          part3: '45 min: Introducci√≥n a Interpretabilidad Mecan√≠stica',
          part4: '30 min: Programaci√≥n: PyTorch y TransformerLens',
          materialsTitle: 'Materiales:',
          handbookTitle: 'Manual:',
          papersTitle: 'Papers:',
          codingTitle: 'Programaci√≥n:',
          videosTitle: 'Videos:'
        },
        session2: {
          title: 'Sesi√≥n 2 - Circuitos',
          date: '30 de agosto, viernes, a partir de las 14:30',
          location: 'Auditorio 418',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulos 3 y 4 del Manual',
          part1: '40 min: Circuitos y el circuito de inducci√≥n',
          part2:
            '20 min: Exploraci√≥n del paper "Un Marco Matem√°tico para Circuitos Transformer"',
          part3: '20 min: Pausa para caf√©',
          part4: '60 min: Programaci√≥n: descubrimiento de circuitos'
        },
        session3: {
          title: 'Sesi√≥n 3 - Superposici√≥n',
          date: '6 de septiembre, viernes, a partir de las 14:30',
          location: 'Auditorio 537',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulo 5 del Manual',
          part1: '30 min: Superposici√≥n',
          part2:
            '30 min: Video de 3Blue1Brown "C√≥mo podr√≠an almacenar hechos los LLMs" y discusi√≥n',
          part3: '20 min: Pausa para caf√©',
          part4: '60 min: Programaci√≥n: superposici√≥n en modelos simplificados'
        },
        session4: {
          title: 'Sesi√≥n 4 - Autoencoders Dispersos (SAE)',
          date: '13 de septiembre, viernes, a partir de las 15:00',
          location: 'Auditorio 418',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulo 6 del Manual',
          part1: '30 min: Autoencoders Dispersos (SAE) y exploraci√≥n pr√°ctica',
          part2:
            '30 min: Exploraci√≥n de los papers "Hacia la Monosem√°ntica: Descomponiendo Modelos de Lenguaje con Aprendizaje de Diccionario" y "Escalando Monosem√°ntica: Extrayendo Caracter√≠sticas Interpretables de Claude 3 Sonnet"',
          part3: '20 min: Pausa para caf√©',
          part4: '40 min: Programaci√≥n: Usando SAEs',
          part5:
            '30 min: ¬°Nuevas √°reas de exploraci√≥n y consejos para el Hackath√≥n!'
        }
      },
      resources: {
        title: 'Recursos Adicionales',
        opportunitiesTitle: 'Oportunidades',
        arenaTitle: 'ARENA',
        arenaDescription:
          'Oportunidad de pasar 4 semanas en Londres estudiando contenido pr√°ctico relevante para la investigaci√≥n en Seguridad de IA',
        arenaButton: 'Saber M√°s',
        matsTitle: 'MATS',
        matsDescription:
          'Programa para iniciar investigaci√≥n en Seguridad de IA con la orientaci√≥n de mentores con amplia experiencia en el campo. Varios autores de art√≠culos utilizados en este curso son mentores de MATS.',
        matsButton: 'Ver Mentores'
      }
    },

    // Handbook common elements
    handbook: {
      tableOfContents: 'Table of Contents',
      prevChapter: '‚Üê Previous Chapter',
      nextChapter: 'Next Chapter ‚Üí',
      noNextChapter: 'End of Handbook',
      furtherReading: 'Further Reading',
      backToHome: '‚Üê Back to Course',

      // Chapter 4: Circuits
      chapter4: {
        title: 'Chapter 4: Circuits',
        subtitle:
          'Understanding circuit-based interpretability in neural networks',
        sections: {
          intro: 'Introduction to Circuits',
          defining: 'Defining Neural Circuits',
          induction: 'The Induction Circuit',
          discovery: 'Circuit Discovery Methods',
          examples: 'Practical Examples',
          conclusion: 'Conclusion'
        },
        intro: {
          p1: 'The "circuits" framework is a key concept in mechanistic interpretability, offering a way to understand how neural networks process information through connected components that collectively implement specific functions.',
          p2: 'This chapter explores how circuit-based interpretability helps us reverse-engineer the computations performed by neural networks, with a focus on transformer architectures used in large language models.'
        },
        defining: {
          p1: 'A neural circuit is a subset of a neural network that implements a specific, identifiable function. Circuits consist of neurons (or groups of neurons) connected across layers that collectively perform a computation.',
          p2: 'Key properties of circuits include:',
          list1:
            'Modularity: Circuits can be studied in isolation from the rest of the network',
          list2:
            'Functionality: Circuits implement specific, meaningful computations',
          list3:
            'Composition: Complex behaviors emerge from combinations of simpler circuits',
          list4:
            'Distributed representation: Information is often represented across multiple neurons rather than in individual neurons'
        },
        induction: {
          p1: 'The induction circuit is one of the most well-studied circuits in transformer models. This circuit enables language models to continue patterns like "A, B, A, B, A, ?" with "B".',
          p2: 'The circuit works by:',
          step1:
            'Detecting when a token has appeared previously in the sequence',
          step2:
            'Identifying what token came after it in that previous occurrence',
          step3:
            'Predicting that the same token will follow in the current context',
          p3: 'This circuit involves specific attention heads working together across layers:',
          component1:
            'Previous token detection: Attention heads that look for repeated tokens',
          component2:
            'Information copying: Mechanisms to retrieve what followed the previous occurrence',
          component3:
            'Output projection: Components that influence the final prediction'
        },
        discovery: {
          p1: 'Identifying circuits within neural networks involves several techniques:',
          method1title: 'Activation Patching',
          method1:
            'This involves replacing activations at specific locations in the network when processing one input with activations from another input, then observing how the output changes. It helps identify which components are causally important for a given task.',
          method2title: 'Causal Mediation Analysis',
          method2:
            'This approach quantifies how much specific paths in the network contribute to a particular prediction, helping identify the most important connections.',
          method3title: 'Attention Analysis',
          method3:
            'Examining attention patterns can reveal which parts of the input sequence influence other parts, providing insights into information flow.'
        },
        examples: {
          p1: 'Beyond induction, researchers have identified several other circuits:',
          circuit1:
            'Indirect Object Identification: Circuits that track relationships between subjects, verbs, and objects in sentences',
          circuit2:
            'Negation Circuits: Components that detect and process negation in text',
          circuit3:
            'Name Mover Circuits: Mechanisms for tracking and referencing entities throughout a text',
          p2: 'These examples demonstrate how complex language understanding emerges from specific circuit implementations.'
        },
        conclusion: {
          p1: 'The circuits framework provides a powerful approach for understanding how neural networks implement specific functions. By identifying and studying these circuits, researchers can gain insights into how models process information and make predictions.',
          p2: 'This understanding is crucial for improving interpretability, enhancing model safety, and developing more reliable AI systems. The next chapter will explore how neural networks handle information when they have limited capacity through the concept of superposition.'
        },
        reading1: 'In-context Learning and Induction Heads',
        reading2: 'A Mathematical Framework for Transformer Circuits',
        reading3: 'Zoom In: An Introduction to Circuits'
      },

      // Chapter 5: Superposition
      chapter5: {
        title: 'Chapter 5: Superposition',
        subtitle:
          'Understanding how neural networks represent more features than they have dimensions',
        sections: {
          intro: 'Introduction to Superposition',
          competition: 'Feature Competition',
          polysemanticity: 'Polysemanticity',
          toyModels: 'Toy Models of Superposition',
          implications: 'Implications for Interpretability',
          conclusion: 'Conclusion'
        },
        intro: {
          p1: 'Superposition refers to the phenomenon where neural networks represent more features than they have neurons or dimensions. This occurs because networks often need to track many more features than they have parameters available to represent them individually.',
          p2: 'The concept was formalized in the paper "Toy Models of Superposition" by Anthropic, which demonstrated how networks can encode multiple features in a lower-dimensional space by exploiting the geometry of feature co-occurrence patterns.'
        },
        competition: {
          p1: 'When a neural network has fewer dimensions than the features it needs to represent, these features must "compete" for representation space. The network learns to allocate its limited representational capacity efficiently.',
          p2: 'Key factors that influence which features get represented include:',
          list1:
            'Frequency: More common features are more likely to be represented',
          list2:
            'Importance: Features that strongly affect the loss function are prioritized',
          list3:
            'Correlation: Features that often co-occur can share representation space',
          list4:
            'Orthogonality: Features that can be represented in orthogonal directions are easier to separate'
        },
        polysemanticity: {
          p1: 'Polysemanticity is a direct consequence of superposition. It refers to the phenomenon where individual neurons or network directions respond to multiple unrelated features.',
          p2: 'In a polysemantic network:',
          list1:
            'Single neurons may respond to multiple semantically distinct concepts',
          list2: 'Feature representations are distributed across many neurons',
          list3:
            'There may not be a clear one-to-one mapping between network components and human-interpretable concepts',
          p3: 'This makes interpretation challenging, as we cannot simply analyze individual neurons to understand what the network is representing.'
        },
        toyModels: {
          p1: 'Researchers have developed simplified models to study superposition. These toy models help illustrate how networks can embed many features in lower-dimensional spaces.',
          p2: 'A typical toy model might involve:',
          step1:
            'Generating synthetic data with a known number of sparse features',
          step2: 'Training a model with fewer dimensions than features',
          step3:
            'Analyzing how the model represents these features in its limited space',
          p3: 'These experiments have revealed that networks can use clever geometric arrangements to encode features efficiently, often exploiting properties like sparsity (features rarely appearing simultaneously).'
        },
        implications: {
          p1: 'Superposition poses several challenges for interpretability research:',
          list1:
            'Direct neuron analysis may reveal misleading or incomplete information',
          list2:
            'Features may be encoded in complex, distributed patterns across many neurons',
          list3:
            'Simple linear probing techniques may fail to detect important features',
          p2: 'However, understanding superposition also provides opportunities:',
          list4:
            'It suggests focusing on finding the right basis for analysis, rather than examining individual neurons',
          list5:
            'Techniques like Sparse Autoencoders (covered in the next chapter) can help extract features from superposed representations',
          list6:
            'Knowledge of superposition patterns can inform better training and architecture design'
        },
        conclusion: {
          p1: 'Superposition is a fundamental property of neural networks that arises when they need to represent more features than they have dimensions. This leads to polysemantic neurons and distributed representations that complicate interpretability efforts.',
          p2: 'Understanding superposition is essential for developing effective methods to interpret neural networks, especially large language models with billions of parameters tracking potentially trillions of features. The next chapter will explore how Sparse Autoencoders can help address this challenge by disentangling these superposed representations.'
        },
        reading1: 'Toy Models of Superposition (Anthropic)',
        reading2:
          'Progress Measures for Grokking via Mechanistic Interpretability',
        reading3: '3Blue1Brown: How LLMs might store facts?'
      },

      // Chapter 6: Sparse Autoencoders
      chapter6: {
        title: 'Chapter 6: Sparse Autoencoders',
        subtitle:
          'Using Sparse Autoencoders to disentangle superposition in neural networks',
        sections: {
          intro: 'Introduction to Sparse Autoencoders',
          architecture: 'Architecture and Training',
          features: 'Feature Extraction',
          applications: 'Applications in Interpretability',
          research: 'Recent Research',
          conclusion: 'Conclusion'
        },
        intro: {
          p1: 'As we saw in the previous chapter, neural networks often represent information in a superposed manner, with many features sharing the same neurons or dimensions. This polysemanticity makes interpretation challenging. Sparse Autoencoders (SAEs) are a powerful tool designed to address this challenge by disentangling these superposed representations.',
          p2: 'The goal of an SAE is to transform the polysemantic, distributed representations in a neural network into a monosemantic, sparse representation, where each feature corresponds to a specific, interpretable concept.'
        },
        architecture: {
          p1: 'A Sparse Autoencoder consists of:',
          list1:
            "An encoder network that maps the original neural network's activations to a higher-dimensional, sparse space",
          list2:
            'A decoder network that reconstructs the original activations from this sparse representation',
          p2: 'The key constraints in training an SAE are:',
          constraint1:
            'Reconstruction loss: The decoder should accurately reconstruct the original activations',
          constraint2:
            'Sparsity constraint: Each input should activate only a small number of features in the encoded representation',
          p3: "These constraints ensure that the SAE learns a dictionary of interpretable features that can be activated individually or in small groups to represent the network's internal states."
        },
        features: {
          p1: 'After training an SAE on a large dataset of activations from a neural network, we can analyze the features it has learned:',
          list1:
            'Each feature can be visualized by examining what types of inputs most strongly activate it',
          list2:
            'Features can be named based on the patterns they recognize (e.g., "quotes detector" or "multiplication operator")',
          list3:
            "The dictionary of features provides a new basis for understanding the network's internal representations",
          p2: "Unlike analyzing individual neurons, SAE features often correspond to meaningful, human-interpretable concepts because they're designed to disentangle the superposed representations."
        },
        applications: {
          p1: 'Sparse Autoencoders can be used for various interpretability tasks:',
          task1title: 'Circuit Discovery',
          task1:
            'By tracking which SAE features activate in response to specific inputs, researchers can identify the computational circuits within the network.',
          task2title: 'Feature Attribution',
          task2:
            'SAEs can help determine which features contribute to specific predictions, providing insight into how the model makes decisions.',
          task3title: 'Editing Model Behavior',
          task3:
            "Once interpretable features are identified, it's possible to modify the model's behavior by intervening on specific features, potentially enabling safer AI systems."
        },
        research: {
          p1: 'Recent advances in SAE research include:',
          advance1:
            'Scaling SAEs to larger models, such as Claude 3 Sonnet by Anthropic',
          advance2:
            'Improving training techniques to identify more interpretable features',
          advance3:
            'Developing automatic methods to name and categorize the discovered features',
          advance4:
            'Using SAEs to understand higher-level abstractions in language models',
          p2: 'Papers like "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning" and "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet" demonstrate the potential of SAEs for understanding increasingly complex models.'
        },
        conclusion: {
          p1: 'Sparse Autoencoders represent one of the most promising approaches for addressing the superposition problem in neural networks. By transforming polysemantic representations into monosemantic features, SAEs provide a powerful tool for mechanistic interpretability.',
          p2: 'As research in this area continues to advance, SAEs may play a crucial role in building more transparent, trustworthy, and alignable AI systems. Understanding the internal workings of neural networks is not just an academic pursuit but a practical necessity as these systems become increasingly powerful and integrated into our society.'
        },
        reading1:
          'Towards Monosemanticity: Decomposing Language Models With Dictionary Learning',
        reading2:
          'Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet',
        reading3: 'SolidGoldMagikarp & Indirect Object Identification'
      }
    }
  },

  es: {
    // Navigation
    nav: {
      home: 'Inicio',
      about: 'Qui√©nes Somos',
      activities: 'Actividades',
      research: 'Investigaci√≥n',
      resources: 'Recursos',
      mechInterpCourse: 'Curso Mech Interp',
      contact: 'Contacto',
      joinUs: 'Sumate'
    },

    // Footer
    footer: {
      privacyPolicy: 'Pol√≠tica de Privacidad',
      copyright: '¬© 2025 BAISH - Buenos Aires AI Safety Hub',
      tagline: 'Buenos Aires AI Safety Hub'
    },

    // Privacy Policy page
    privacy: {
      pageHeader: {
        breadcrumb: 'Pol√≠tica de Privacidad',
        title: 'Pol√≠tica de Privacidad'
      },
      intro: {
        title: 'Nuestro Enfoque de Privacidad',
        content:
          'En BAISH (Buenos Aires AI Safety Hub), estamos comprometidos a respetar tu privacidad. Esta Pol√≠tica de Privacidad describe nuestras pr√°cticas con respecto a la recopilaci√≥n, uso y protecci√≥n de tu informaci√≥n cuando utilizas nuestro sitio web y servicios.'
      },
      dataCollection: {
        title: 'Recopilaci√≥n de Datos',
        content:
          'Recopilamos informaci√≥n personal m√≠nima. Los √∫nicos datos personales que recopilamos son las direcciones de correo electr√≥nico cuando los usuarios se registran voluntariamente para recibir nuestro bolet√≠n a trav√©s de Substack. Esta informaci√≥n es almacenada y gestionada por Substack de acuerdo con su pol√≠tica de privacidad.'
      },
      noTracking: {
        title: 'Sin Seguimiento ni Cookies',
        content:
          'No utilizamos cookies, an√°lisis, herramientas de seguimiento ni ninguna otra tecnolog√≠a para recopilar datos sobre ti. No monitoreamos tus actividades de navegaci√≥n ni recopilamos informaci√≥n sobre tus comportamientos en l√≠nea.'
      },
      thirdParty: {
        title: 'Servicios de Terceros',
        content:
          'Nuestro bolet√≠n se gestiona a trav√©s de Substack. Cuando te suscribes a nuestro bolet√≠n, tu direcci√≥n de correo electr√≥nico se comparte y almacena con Substack. Consulta la Pol√≠tica de Privacidad de Substack para comprender c√≥mo manejan tu informaci√≥n.'
      },
      yourRights: {
        title: 'Tus Derechos',
        content:
          'Tienes derecho a cancelar la suscripci√≥n a nuestro bolet√≠n en cualquier momento haciendo clic en el enlace para cancelar la suscripci√≥n en cualquiera de nuestros correos electr√≥nicos o contact√°ndonos directamente. Si tienes alguna pregunta sobre tus datos o deseas acceder, corregir o eliminar tu informaci√≥n, cont√°ctanos.'
      },
      changes: {
        title: 'Cambios a Esta Pol√≠tica',
        content:
          'Podemos actualizar esta Pol√≠tica de Privacidad de vez en cuando. Te notificaremos cualquier cambio publicando la nueva Pol√≠tica de Privacidad en esta p√°gina.'
      },
      contact: {
        title: 'Cont√°ctanos',
        content:
          'Si tienes alguna pregunta sobre esta Pol√≠tica de Privacidad, cont√°ctanos a trav√©s de nuestra p√°gina de Contacto.'
      }
    },

    // Home page
    home: {
      hero: {
        title: 'Asegurando que la IA Beneficie a la Humanidad',
        subtitle:
          'Iniciativa estudiantil de BAISH explorando el desarrollo seguro y beneficioso de la IA',
        paragraph1_part1:
          'A medida que los modelos de inteligencia artificial avanzan en capacidades',
        paragraph1_part2:
          ', esperamos que tengan un impacto cada vez m√°s profundo en nuestra sociedad',
        paragraph1_part3:
          '. Es esencial que este impacto sea positivo, y que las decisiones tomadas por estos sistemas sean transparentes, confiables y responsables',
        paragraph1_part4: ' ante las personas afectadas por ellos.',
        paragraph2_part1:
          'Creemos que reducir los riesgos asociados a modelos avanzados de IA',
        paragraph2_part2:
          ' es uno de los desaf√≠os m√°s importantes de nuestro tiempo. Tambi√©n creemos que es un problema abierto y apasionante',
        paragraph2_part3:
          ', con amplias oportunidades para que m√°s investigadores avancen en este campo',
        paragraph2_part4: '.',
        paragraph3:
          'La misi√≥n de BAISH es apoyar a estudiantes a entrar a este campo y a realizar investigaciones sobre este tema.',
        cta: 'Participa'
      },
      intro: {
        title: '¬øPor qu√© es Importante la Seguridad de la IA?',
        paragraph1:
          'Los sistemas de IA avanzados son cada vez m√°s capaces, con el potencial de transformar la sociedad de manera profunda. El campo de la Seguridad de la IA se enfoca en garantizar que estos sistemas est√©n alineados con los valores humanos y sean beneficiosos para la humanidad.',
        paragraph2:
          'En BAISH - Buenos Aires AI Safety Hub, reunimos a estudiantes de Ciencias de la Computaci√≥n, F√≠sica , Matem√°ticas y demas entusiastas para entender y abordar estos desaf√≠os a trav√©s de la investigaci√≥n, la educaci√≥n y la comunidad.',
        point1: 'Entendiendo las capacidades y riesgos de la IA',
        point2: 'Desarrollando t√©cnicas de alineamiento',
        point3: 'Realizando investigaci√≥n original',
        point4: 'Construyendo una comunidad de investigadores',
        learnMore: 'M√°s sobre Seguridad en IA ‚Üí'
      },
      upcomingEvent: {
        title: 'Grupo de Discusi√≥n Semanal: M√©todos de Interpretabilidad',
        eventType: 'Pr√≥ximo Evento',
        time: '18:00 - 19:30',
        location: 'Pabell√≥n 0+inf, Aula 1603, Ciudad Universitaria',
        description:
          'Sumate a una discusi√≥n interactiva sobre avances recientes en m√©todos de interpretabilidad de redes neuronales y sus implicaciones para el alineamiento de la IA.',
        rsvp: 'Agregalo a tu calendario'
      },
      activities: {
        title: 'Nuestras Actividades',
        card1: {
          title: 'Cohorte de Fundamentos de Seguridad en AGI',
          description:
            'Un curso guiado de 8 semanas cubriendo los conceptos esenciales en alineamiento y seguridad de IA. Los participantes leen materiales seleccionados y se re√∫nen semanalmente para discutir las lecturas con un facilitador.',
          status: 'Comienza en el segundo semestre',
          link: 'Ver M√°s ‚Üí'
        },
        card2: {
          title: 'Grupo de Discusi√≥n Semanal',
          description:
            'Reuniones semanales para discutir papers recientes, conceptos y avances en seguridad de IA.',
          status: 'Todos los martes a las 14:30',
          link: 'Saber M√°s ‚Üí'
        },
        card3: {
          title: 'Club de Lectura de Papers',
          description:
            'Presentaciones de estudiantes sobre papers de Seguridad en IA.',
          status: 'Todos los martes a las 14:00',
          link: 'Saber M√°s ‚Üí'
        },
        card4: {
          title: 'Curso de Interpretabilidad Mecan√≠stica',
          description:
            'Curso intensivo de 1 mes sobre m√©todos de interpretabilidad mec√°nica.',
          status: 'Comienza en fines de Abril 2025',
          link: 'Saber M√°s ‚Üí'
        },
        discussion: {
          title: 'Grupo de Discusi√≥n Semanal',
          description:
            'Sumate a nuestro grupo de discusi√≥n semanal sobre temas de seguridad de IA y avances recientes del campo.',
          addToCalendar: 'Agregalo a tu calendario'
        }
      },
      getInvolved: {
        title: 'Sumate',
        mailingList: {
          title: 'Suscribite a Nuestra Lista de Correo',
          description:
            'Mantenete al d√≠a con nuestras actividades, eventos y oportunidades.'
        },
        telegram: {
          title: 'Sumate a Nuestra Comunidad de Telegram',
          description:
            'Conectate con otros estudiantes interesados en la seguridad de la IA.',
          button: 'Sumate al grupo de Telegram'
        }
      },
      events: {
        // Added new events section
        title: 'Pr√≥ximos Eventos',
        description:
          'Sumate a nuestros pr√≥ximos eventos, discusiones y talleres sobre seguridad de la IA. Todos los eventos est√°n abiertos a la comunidad.',
        subscribeCalendar: 'Suscribirse al Calendario'
      }
    },

    // Hackathon translations
    hackathon: {
      title: 'AI Safety Hackathon: Model Routing',
      subtitle: 'Beyond Single Models - Revoluci√≥n del Model Routing',
      prize: '$2,000 USD en Premios',
      days: 'D√≠as',
      hours: 'Horas',
      minutes: 'Min',
      seconds: 'Seg',
      register: '¬°Inscribite ahora!',
      urgency: '¬°Cupos limitados - No te quedas afuera!',
      eventTitle: 'AI Safety Hackathon - Model Routing',
      eventDate: '30 de Mayo - 1 de Junio, 2025',
      venue: '@ Aleph Hub',
      foodIncluded: 'ü•ó Comida vegana incluida',
      eventDescription:
        'Sumate al primer hackathon de AI Safety de Argentina. Exploraremos t√©cnicas revolucionarias de Model Routing que van m√°s all√° de modelos √∫nicos para crear sistemas de IA m√°s seguros y eficientes. ¬°Compet√≠ por $2,000 USD en premios!',
      registerEvent: '¬°Reg√≠strate ahora!',
      moreInfo: 'M√°s Informaci√≥n & Registro ‚Üí',
      tracks: {
        title: 'Tracks de Desaf√≠o',
        track1: {
          title: 'Desarrollo de Modelo de Evaluaci√≥n',
          description:
            'Construye modelos de evaluaci√≥n especializados que eval√∫an capacidades de IA en dimensiones relevantes para la seguridad.'
        },
        track2: {
          title: 'Sistemas de Enrutamiento Inteligente',
          description:
            'Desarrolla sistemas que dirigen consultas de manera inteligente a modelos especializados apropiados.'
        },
        track3: {
          title: 'Descomposici√≥n de Tareas',
          description:
            'Crea marcos de trabajo que descomponen solicitudes complejas en pasos manejables para diferentes modelos.'
        },
        track4: {
          title: 'Integraci√≥n de Modelos',
          description:
            'Construye marcos de trabajo para una integraci√≥n f√°cil de modelos especializados en la arquitectura de enrutamiento.'
        }
      },
      feature1: {
        title: 'Premios Atractivos',
        description: '$2,000 USD distribuidos entre los mejores proyectos'
      },
      feature2: {
        title: 'Model Routing',
        description: 'T√©cnicas innovadoras para sistemas multi-modelo'
      },
      feature3: {
        title: 'Apoyo',
        description: 'Apoyo durante el fin de semana'
      }
    },

    // About page
    about: {
      pageHeader: {
        breadcrumb: 'Inicio / Seguridad en IA',
        title: 'Entendiendo la Seguridad en IA'
      },
      coreConcepts: {
        whatIs: {
          title: '¬øQu√© es la Seguridad en IA?',
          content:
            'La Seguridad en IA es un campo de investigaci√≥n enfocado en asegurar que los sistemas avanzados de inteligencia artificial sigan siendo beneficiosos, alineados con los valores humanos y bajo control humano a medida que se vuelven m√°s capaces. Abarca √°reas t√©cnicas de investigaci√≥n como alineamiento, interpretabilidad y robustez, as√≠ como consideraciones de gobernanza sobre c√≥mo deber√≠an desarrollarse e implementarse los sistemas de IA.'
        },
        whyMatters: {
          title: '¬øPor qu√© es Importante?',
          content:
            'A medida que los sistemas de IA se vuelven m√°s poderosos y aut√≥nomos, pueden desarrollar capacidades que podr√≠an llevar a consecuencias no deseadas si no est√°n dise√±ados y controlados adecuadamente. Lo que est√° en juego es importante: la IA avanzada podr√≠a ayudar a resolver los mayores desaf√≠os de la humanidad, pero tambi√©n presenta riesgos significativos si se desarrolla sin medidas de seguridad adecuadas. El campo busca maximizar los beneficios mientras minimiza los da√±os potenciales.'
        },
        challenges: {
          title: 'Riesgos y Desaf√≠os Principales',
          alignment: {
            title: 'Problema de Alineamiento',
            content:
              'Asegurar que los sistemas de IA persigan objetivos alineados con los valores e intenciones humanas, incluso cuando se vuelven m√°s capaces.'
          },
          interpretability: {
            title: 'Interpretabilidad',
            content:
              'Desarrollar t√©cnicas para entender c√≥mo los sistemas de IA toman decisiones y representan conocimiento.'
          },
          robustness: {
            title: 'Robustez',
            content:
              'Crear sistemas que se comporten de manera segura incluso cuando se implementan en nuevos entornos o enfrentan situaciones inesperadas.'
          },
          powerSeeking: {
            title: 'Comportamiento de B√∫squeda de Poder',
            content:
              'Prevenir que los sistemas de IA desarrollen objetivos instrumentales que entren en conflicto con el bienestar humano.'
          },
          coordination: {
            title: 'Desaf√≠os de Coordinaci√≥n',
            content:
              'Garantizar que se mantengan est√°ndares de seguridad en todos los principales esfuerzos de desarrollo de IA a nivel mundial.'
          }
        }
      },
      externalResources: {
        title: 'Aprend√© M√°s Sobre Seguridad en IA',
        cards: {
          alignmentForum: {
            title: 'Alignment Forum',
            description:
              'Un foro dedicado a la investigaci√≥n t√©cnica en alineamiento de IA, con papers y discusiones de investigadores destacados.',
            difficulty: 'T√©cnico',
            cta: 'Visitar'
          },
          lessWrong: {
            title: 'LessWrong',
            description:
              'Un blog comunitario enfocado en la racionalidad humana y las implicaciones de la inteligencia artificial.',
            difficulty: 'Intermedio',
            cta: 'Visitar'
          },
          eightyThousandHours: {
            title: '80,000 Hours',
            description:
              'Orientaci√≥n profesional para trabajar en los problemas m√°s urgentes del mundo, incluida la seguridad de la IA.',
            difficulty: 'Introductorio',
            cta: 'Visitar'
          },
          stampy: {
            title: 'Wiki de Stampy',
            description:
              'Una wiki colaborativa que proporciona explicaciones accesibles de conceptos de alineamiento de IA.',
            difficulty: 'Introductory',
            cta: 'Visitar'
          }
        }
      },
      ourApproach: {
        title: 'Nuestro Enfoque',
        focusAreas: {
          title: '√Åreas de Enfoque',
          intro:
            'En BAISH - Buenos Aires AI Safety Hub, nos enfocamos en varias √°reas clave dentro de la investigaci√≥n de seguridad en IA:',
          areas: [
            'Interpretabilidad mec√°nica de redes neuronales',
            'T√©cnicas de alineamiento para modelos grandes de lenguaje',
            'Metodolog√≠as de entrenamiento robustas',
            'Aprendizaje de valores e inferencia de preferencias'
          ]
        },
        contributions: {
          title: 'Nuestra Contribuci√≥n',
          intro: 'Contribuimos al campo a trav√©s de:',
          items: [
            'Apoyando proyectos de investigaci√≥n estudiantil',
            'Desarrollando recursos educativos en espa√±ol',
            'Construyendo una comunidad regional de investigadores en seguridad de IA',
            'Organizando talleres y programas de formaci√≥n',
            'Mentoreando a estudiantes interesados en carreras de seguridad en IA'
          ]
        }
      },
      team: {
        title: 'Equipo Principal',
        members: {
          eitan: {
            name: 'Eitan Sprejer',
            title: 'Co-director Fundador',
            bio: 'Estudiante de F√≠sica con pasi√≥n por la seguridad y el alineamiento de la IA.'
          },
          luca: {
            name: 'Luca De Leo',
            title: 'Co-director Fundador',
            bio: 'Generalista de Operaciones con experiencia trabajando en Organizaciones de Seguridad en IA'
          },
          lucas: {
            name: 'Lucas Vitali',
            title: 'Director de Comunicaci√≥n',
            bio: 'Estudiante de Ciencias de la Computaci√≥n con pasi√≥n por la comunicaci√≥n cient√≠fica'
          },
          sergio: {
            name: 'Sergio Abriola, PhD',
            title: 'Advisor',
            bio: 'Doctor en Matem√°ticas y profesor con pasi√≥n por la seguridad en IA'
          }
        }
      }
    },

    // Activities page
    activities: {
      pageHeader: {
        breadcrumb: 'Inicio / Actividades',
        title: 'Nuestras Actividades',
        subtitle:
          'Sumate a nuestra comunidad y particip√° en investigaci√≥n y aprendizaje sobre seguridad en IA'
      },
      agiFundamentals: {
        title: 'Cohorte de Fundamentos de Seguridad en AGI',
        status: 'Comienza en el segundo semestre',
        description:
          'La cohorte de Fundamentos de Seguridad en AGI es un curso guiado de 8 semanas que cubre los conceptos esenciales en alineamiento y seguridad de IA. Los participantes leen materiales seleccionados y se re√∫nen semanalmente para discutir las lecturas con un facilitador.',
        description2:
          'Este programa se basa en el curr√≠culo de Fundamentos de Seguridad en AGI de BlueDot y proporciona una introducci√≥n estructurada al campo de la seguridad en IA.',
        whatToExpect: {
          title: 'Qu√© Esperar',
          items: [
            'Sesiones de discusi√≥n semanales de 2 horas',
            '1-3 horas de lectura por semana',
            'Grupos peque√±os de 6-12 participantes',
            'Facilitadores experimentados para guiar las discusiones',
            'Certificado de finalizaci√≥n'
          ]
        },
        details: {
          title: 'Detalles del Programa',
          duration: 'Duraci√≥n:',
          durationValue: '10-12 semanas',
          period: 'Per√≠odo de Fellowship:',
          periodValue: 'Agosto - Diciembre 2025'
        },
        viewCurriculumButton: 'Ver Plan de Estudios'
      },
      weeklyDiscussion: {
        title: 'Grupo de Discusi√≥n Semanal',
        status: 'Todos los martes a las 14:30',
        description:
          'Nuestro Grupo de Discusi√≥n Semanal proporciona un foro casual para discutir papers recientes, conceptos y avances en seguridad de IA. Estas sesiones est√°n abiertas a cualquier persona interesada en el campo, independientemente de conocimientos previos.',
        description2:
          'Cada semana presenta un tema diferente, anunciado con anticipaci√≥n a trav√©s de nuestra lista de correo y grupo de Telegram.',
        format: {
          title: 'Formato',
          items: [
            'Discusiones de 90 minutos dirigidas por un facilitador rotativo',
            'Breve presentaci√≥n del tema de la semana (15-20 minutos)',
            'Discusi√≥n abierta y preguntas y respuestas',
            'Materiales de lectura previa opcionales compartidos con anticipaci√≥n'
          ]
        },
        participation: {
          title: 'Participaci√≥n',
          description:
            'No se requiere inscripci√≥n. ¬°Simplemente ven√≠! Si asist√≠s por primera vez, te recomendamos llegar 10 minutos antes para conocer a los organizadores.'
        },
        nextDiscussion: {
          title: 'Pr√≥xima Discusi√≥n',
          date: 'Fecha:',
          dateValue: '25 de Marzo, 2025',
          time: 'Hora:',
          timeValue: '18:00 - 19:30',
          location: 'Ubicaci√≥n:',
          locationValue: 'Pabell√≥n 0+inf, Aula 1603, Ciudad Universitaria',
          topic: 'Tema:',
          topicValue: 'M√©todos de Interpretabilidad',
          facilitator: 'Facilitador:',
          facilitatorValue: 'Eitan Sprejer',
          button: 'Sumate al Telegram para Actualizaciones'
        }
      },
      paperReading: {
        title: 'Club de Lectura de Papers',
        status: 'Todos los martes a las 14:00',
        description:
          'Nuestro Club de Lectura de Papers se enfoca en leer y discutir papers de investigaci√≥n recientes en seguridad de IA. Los participantes se turnan para presentar papers y liderar discusiones sobre las implicaciones de la investigaci√≥n.',
        description2:
          'Se espera que los participantes lean el paper seleccionado con antelaci√≥n y vengan preparados para discutir sus m√©todos, resultados e implicaciones.',
        selectionCriteria: {
          title: 'Criterios de Selecci√≥n de Papers',
          items: [
            'Importancia para el campo de la seguridad en IA',
            'Relevancia t√©cnica para las direcciones de investigaci√≥n actuales',
            'Mezcla de papers fundamentales y publicaciones recientes',
            'Accesibilidad para estudiantes de posgrado y de grado avanzados'
          ]
        },
        sessionFormat: {
          title: 'Formato de la Sesi√≥n',
          items: [
            'Breve descripci√≥n general del paper (5-10 minutos)',
            'Discusi√≥n secci√≥n por secci√≥n',
            'Examen de m√©todos y resultados',
            'Evaluaci√≥n cr√≠tica de afirmaciones y limitaciones',
            'Discusi√≥n de posible investigaci√≥n de seguimiento'
          ]
        },
        nextSession: {
          title: 'Pr√≥xima Sesi√≥n de Paper',
          date: 'Fecha:',
          dateValue: '21 de Marzo, 2025',
          time: 'Hora:',
          timeValue: '17:00 - 18:30',
          location: 'Ubicaci√≥n:',
          locationValue: 'Pabell√≥n 0+inf, Aula 1604, Ciudad Universitaria',
          paper: 'Paper:',
          paperValue:
            '"Interpretabilidad Mecan√≠stica para Modelos de Lenguaje"',
          lead: 'L√≠der de Discusi√≥n:',
          leadValue: 'Eitan Sprejer',
          button: 'Acceder a la Lista de Lectura'
        }
      },
      mechInterp: {
        title: 'Curso de Interpretabilidad Mecan√≠stica',
        status: 'Comienza en fines de Abril 2025',
        description:
          'El Curso de Interpretabilidad Mecan√≠stica es un programa intensivo de 1 mes enfocado en t√©cnicas para entender los mecanismos internos de las redes neuronales. Este curso combina aprendizaje te√≥rico con proyectos pr√°cticos.',
        description2:
          'La interpretabilidad mec√°nica es un √°rea clave de la investigaci√≥n en seguridad de IA, que busca hacer que los sistemas de IA sean m√°s transparentes y comprensibles.',
        curriculum: {
          title: 'Descripci√≥n General del Plan de Estudios',
          items: [
            'Fundamentos de arquitecturas de redes neuronales',
            'T√©cnicas de visualizaci√≥n de caracter√≠sticas',
            'An√°lisis de circuitos en modelos de transformers',
            'T√©cnicas para analizar mecanismos de atenci√≥n',
            'M√©todos de atribuci√≥n basados en gradientes',
            'Proyecto final: Interpretaci√≥n de un componente espec√≠fico del modelo'
          ]
        },
        timeCommitment: {
          title: 'Dedicaci√≥n de Tiempo',
          items: [
            '2 clases por semana (2 horas cada una)',
            '1 sesi√≥n pr√°ctica por semana (3 horas)',
            'Trabajo de proyecto individual (5-10 horas por semana)',
            'Presentaci√≥n de proyecto final'
          ]
        },
        prerequisites: {
          title: 'Requisitos Previos',
          items: [
            'S√≥lidas habilidades de programaci√≥n (Python)',
            'Experiencia con frameworks de aprendizaje profundo (preferiblemente PyTorch)',
            'Familiaridad con arquitecturas b√°sicas de redes neuronales',
            '√Ålgebra lineal y c√°lculo'
          ]
        },
        courseDetails: {
          title: 'Detalles del Curso',
          duration: 'Duraci√≥n:',
          durationValue: '4 semanas',
          startDate: 'Fecha de Inicio:',
          startDateValue: '2 de Junio, 2025',
          endDate: 'Fecha de Finalizaci√≥n:',
          endDateValue: '27 de Junio, 2025',
          deadline: 'Fecha L√≠mite de Solicitud:',
          deadlineValue: '15 de Mayo, 2025',
          location: 'Ubicaci√≥n:',
          locationValue: 'H√≠brido (Presencial y Zoom)',
          instructors: 'Instructores:',
          instructorsValue: 'Dra. Laura Fernandez, Carlos Mendez',
          button: 'Expresar Inter√©s'
        },
        viewCourseButton: 'Ver Informaci√≥n Detallada del Curso'
      },
      calendar: {
        title: 'Pr√≥ximos Eventos',
        description:
          'Sumate a nuestros pr√≥ximos eventos de seguridad en IA, discusiones y talleres. Todos los eventos est√°n abiertos a la comunidad.',
        subscribeLuma: 'Suscribirse al Calendario de Eventos'
      }
    },

    // Resources page
    resources: {
      pageHeader: {
        breadcrumb: 'Inicio / Recursos',
        title: 'Recursos de Aprendizaje',
        subtitle:
          'Materiales curados para explorar conceptos de seguridad en IA y nuestro archivo de boletines'
      },
      learningPathways: {
        title: 'Caminos de Aprendizaje',
        intro:
          'Hemos organizado recursos en tres caminos de aprendizaje seg√∫n tu formaci√≥n y nivel de experiencia.',
        pathways: {
          beginners: {
            title: 'Principiantes',
            difficulty: 'Accesible para Todos',
            description:
              'Para quienes son nuevos en los conceptos de seguridad en IA y quieren entender las ideas y preocupaciones principales.',
            resources: {
              resource1: {
                title: 'Future of Life Institute: Introducci√≥n a Riesgos de IA',
                type: 'Panorama General'
              },
              resource2: {
                title: 'Cold Takes: Serie sobre IA Transformativa',
                type: 'Serie de Blog'
              },
              resource3: {
                title: 'Rob Miles: Intro a Seguridad en IA',
                type: 'Video'
              },
              resource4: {
                title: '80,000 Hours: Perfil del Problema de Riesgo de IA',
                type: 'Panorama General'
              },
              resource5: {
                title: 'Wiki de Seguridad en IA de Stampy',
                type: 'Wiki'
              }
            },
            button: 'Obtener Recomendaciones para Principiantes'
          },
          intermediate: {
            title: 'Intermedio',
            difficulty: 'Algo de Formaci√≥n T√©cnica',
            description:
              'Para quienes tienen alg√∫n conocimiento de aprendizaje autom√°tico y quieren explorar los desaf√≠os centrales de la seguridad en IA.',
            resources: {
              resource1: {
                title: 'Plan de Estudios de Fundamentos de Seguridad en AGI',
                type: 'Materiales de Curso'
              },
              resource2: {
                title: 'Fundamentos de Alineamiento T√©cnico',
                type: 'Lista de Lectura'
              },
              resource3: {
                title: 'Anthropic: Visiones Centrales sobre Seguridad en IA',
                type: 'Perspectiva de Empresa'
              },
              resource4: {
                title:
                  'Entrenando Modelos de Lenguaje para Seguir Instrucciones',
                type: 'Paper de Investigaci√≥n'
              },
              resource5: {
                title: 'DeepLearning.AI: T√©cnicas de Alineamiento para LLMs',
                type: 'Curso'
              }
            },
            button: 'Obtener Recomendaciones Intermedias'
          },
          advanced: {
            title: 'Avanzado',
            difficulty: 'S√≥lida Formaci√≥n T√©cnica',
            description:
              'Para quienes tienen s√≥lidos conocimientos de ML/IA y quieren involucrarse en investigaci√≥n de vanguardia.',
            resources: {
              resource1: {
                title: 'Distill: Hilo de Circuitos',
                type: 'Investigaci√≥n de Interpretabilidad'
              },
              resource2: {
                title: 'Anthropic: Circuitos de Transformer',
                type: 'Serie de Investigaci√≥n'
              },
              resource3: {
                title:
                  'Descubriendo Conocimiento Latente en Modelos de Lenguaje',
                type: 'Paper de Investigaci√≥n'
              },
              resource4: {
                title: 'Agentes de Lenguaje para Investigaci√≥n de Alineamiento',
                type: 'Paper de Investigaci√≥n'
              },
              resource5: {
                title: 'Alignment Forum: Conceptos Avanzados',
                type: 'Discusi√≥n T√©cnica'
              }
            },
            button: 'Obtener Recomendaciones Avanzadas'
          }
        }
      },
      recommendedBooks: {
        title: 'Libros Recomendados',
        books: {
          book1: {
            title: 'Uncontrollable',
            author: 'Darren McKee (2023)',
            description:
              'Uncontrollable utiliza analog√≠as atractivas y ejemplos relacionables para resumir la IA para principiantes, y explica el riesgo y la seguridad de la IA para lectores sin formaci√≥n t√©cnica.'
          },
          book2: {
            title: 'Human Compatible',
            author: 'Stuart Russell (2019)',
            description:
              "A leading AI researcher's compelling case for how to ensure that artificial intelligence remains beneficial to humanity."
          },
          book3: {
            title: 'Superintelligence',
            author: 'Nick Bostrom (2014)',
            description:
              'Explores the potential risks and paths to superintelligent AI, addressing key questions about the future of intelligence.'
          },
          book4: {
            title: 'The Alignment Problem',
            author: 'Brian Christian (2020)',
            description:
              'An exploration of the growing field of AI alignment, explaining technical concepts in an accessible way.'
          },
          book5: {
            title: 'Life 3.0',
            author: 'Max Tegmark (2017)',
            description:
              'Examines how artificial intelligence might affect life in the future and what choices we face in shaping that future.'
          }
        }
      }
    },

    // Contact pages
    contact: {
      pageHeader: {
        breadcrumb: 'Inicio / Contacto',
        title: 'Contactanos',
        subtitle: 'Comunicate con la comunidad de BAISH'
      },
      contactInfo: {
        email: {
          title: 'Email',
          description: 'Para consultas generales, sugerencias o preguntas:'
        },
        telegram: {
          title: 'Telegram',
          description:
            'Sumate a nuestro canal de comunidad para discusiones y actualizaciones:'
        },
        location: {
          title: 'Ubicaci√≥n',
          description:
            'Estamos ubicados en el Departamento de Ciencias de la Computaci√≥n:'
        },
        socialMedia: {
          title: 'Redes Sociales',
          description: 'Seguinos para actualizaciones y anuncios:'
        }
      },
      getInvolved: {
        title: 'Sumate',
        intro:
          'Hay m√∫ltiples formas de participar en nuestra comunidad y actividades.',
        mailingList: {
          title: 'Suscribite a Nuestra Lista de Correo',
          description:
            'Mantenete al d√≠a con nuestros eventos, actividades y oportunidades suscribi√©ndote a nuestra lista de correo. Te enviamos newsletters mensuales y anuncios importantes.'
        }
      },
      form: {
        name: '¬øCu√°l es tu nombre?',
        email: '¬øCu√°l es tu correo electr√≥nico?',
        message: 'Mensaje',
        clear: 'Limpiar formulario',
        submit: 'Enviar'
      },
      faq: {
        title: 'Preguntas Frecuentes',
        questions: {
          q1: {
            question: '¬øNecesito ser estudiante de la UBA para participar?',
            answer:
              'La mayor√≠a de nuestras actividades est√°n principalmente dise√±adas para estudiantes de la UBA, pero estamos abiertos a participantes de otras universidades para nuestras reuniones de grupo y sesiones de lectura de papeles. Los becarios de investigaci√≥n actualmente est√°n limitados a estudiantes de la UBA.'
          },
          q2: {
            question: '¬øQu√© antecedentes necesito para participar?',
            answer:
              'Esto var√≠a por actividad. Nuestras reuniones de grupo est√°n abiertas a cualquier persona con inter√©s en la seguridad en IA, independientemente del fondo t√©cnico. Para actividades m√°s t√©cnicas como el curso de Interpretabilidad Mecan√≠stica o el becario de investigaci√≥n, se espera alg√∫n conocimiento en ciencias de la computaci√≥n, matem√°ticas o IA/ML.'
          },
          q3: {
            question: '¬øSe realizan tus actividades en ingl√©s o espa√±ol?',
            answer:
              'Realizamos la mayor√≠a de nuestras actividades en ambos idiomas. Las reuniones de grupo son t√≠picamente en espa√±ol, mientras que algunas sesiones t√©cnicas pueden ser en ingl√©s, especialmente cuando se utilizan materiales de fuentes internacionales. Nuestros materiales escritos est√°n disponibles en ambos idiomas siempre que sea posible.'
          },
          q4: {
            question:
              '¬øC√≥mo puedo empezar a aprender sobre seguridad en IA si soy un principiante completo?',
            answer:
              'Te recomendamos empezar con nuestros recursos principiantes en la p√°gina de Recursos y unirte a nuestro grupo de reuniones de grupo. El grupo de reuniones de grupo es un gran lugar para hacer preguntas y aprender de otros en un entorno informal.'
          },
          q5: {
            question:
              '¬øPuedo proponer una nueva actividad o direcci√≥n de investigaci√≥n?',
            answer:
              '¬°Por supuesto! Estamos siempre abiertos a nuevas ideas. Cont√°ctanos en aisafetyarg@gmail.com con tu propuesta y uno de nuestros coordinadores te lo discutir√° con vos.'
          }
        }
      }
    },

    // Research page
    research: {
      pageHeader: {
        breadcrumb: 'Investigaci√≥n',
        title: 'Nuestra Investigaci√≥n',
        subtitle:
          'Proyectos dirigidos por estudiantes que exploran desaf√≠os cr√≠ticos de la seguridad de la IA'
      },
      overview: {
        approach: {
          title: 'Enfoque de Investigaci√≥n',
          paragraph1:
            'En BAISH - Buenos Aires AI Safety Hub, apoyamos la investigaci√≥n liderada por estudiantes sobre los desaf√≠os fundamentales de la seguridad de la IA. Nuestros proyectos van desde el trabajo t√©cnico en interpretabilidad y alineaci√≥n hasta exploraciones m√°s conceptuales de gobernanza y √©tica de la IA.',
          paragraph2:
            'Los proyectos de investigaci√≥n se realizan t√≠picamente a trav√©s de nuestro programa de Becas de Investigaci√≥n o como colaboraciones entre estudiantes y profesores. Fomentamos m√©todos rigurosos, pensamiento creativo y enfoques interdisciplinarios.'
        },
        focusAreas: {
          title: '√Åreas de Enfoque',
          interpretability:
            '<strong>Interpretabilidad y Transparencia</strong> - Entender c√≥mo las redes neuronales representan y procesan informaci√≥n',
          alignment:
            '<strong>T√©cnicas de Alineaci√≥n</strong> - Desarrollar m√©todos para alinear sistemas de IA con valores e intenciones humanas',
          robustness:
            '<strong>Robustez</strong> - Crear sistemas de IA que mantengan un comportamiento seguro en nuevos entornos',
          valueLearning:
            '<strong>Aprendizaje de Valores</strong> - Inferir preferencias humanas a partir de retroalimentaci√≥n y demostraci√≥n'
        }
      },
      projects: {
        title: 'Proyectos de Investigaci√≥n',
        filterBy: 'Filtrar por:',
        filters: {
          all: 'Todos',
          interpretability: 'Interpretabilidad',
          alignment: 'Alineaci√≥n',
          robustness: 'Robustez',
          valueLearning: 'Aprendizaje de Valores'
        }
      },
      publications: {
        title: 'Publicaciones',
        intro:
          'Publicaciones seleccionadas de nuestros miembros en revistas revisadas por pares.'
      },
      ongoingResearch: {
        title: 'Investigaci√≥n en Curso',
        intro:
          'Proyectos actuales en desarrollo por nuestros becarios de investigaci√≥n y colaboradores.'
      },
      joinResearch: {
        title: '¬øInteresado en Realizar Investigaci√≥n con Nosotros?',
        description:
          'Damos la bienvenida a estudiantes de Ciencias de la Computaci√≥n, Matem√°ticas, F√≠sica y campos relacionados para unirse a nuestros esfuerzos de investigaci√≥n. Ya sea que seas un estudiante de pregrado buscando experiencia en investigaci√≥n o un estudiante de posgrado interesado en la seguridad de la IA, tenemos oportunidades para ti.',
        buttons: {
          fellowships: 'Conoce las Becas de Investigaci√≥n',
          contact: 'Contacta a los Coordinadores de Investigaci√≥n'
        }
      }
    },

    // Common elements
    common: {
      learnMore: 'Ver M√°s',
      readMore: 'Leer M√°s',
      upcoming: 'Pr√≥ximamente',
      active: 'Activo',
      completed: 'Finalizado'
    },

    // Mech Interp Course page
    mechInterpCourse: {
      pageHeader: {
        title: 'Curso de Interpretabilidad Mecan√≠stica',
        subtitle:
          'Una introducci√≥n completa a la Interpretabilidad Mecan√≠stica para Modelos de Lenguaje Grande'
      },
      backToHome: '‚Üê Volver al Inicio',
      intro: {
        paragraph1:
          'Este es un curso r√°pido de 4 sesiones dise√±ado para introducir el campo de la Interpretabilidad Mecan√≠stica para Modelos de Lenguaje Grandes (LLMs). El curso incluye materiales te√≥ricos, introduce librer√≠as de Python para interpretabilidad, discute art√≠culos recientes en el campo (publicados por organizaciones como Anthropic y Google DeepMind), y proporciona ejercicios pr√°cticos.',
        paragraph2:
          'El campo de la Interpretabilidad Mecan√≠stica ha ganado r√°pidamente popularidad en los √∫ltimos a√±os, con m√°s de 90 papers aceptados en ICML 2024. Su objetivo principal es entender la l√≥gica detr√°s de las decisiones de los modelos de aprendizaje autom√°tico. Este conocimiento puede aplicarse para mejorar la transparencia y confianza en modelos existentes, as√≠ como para entender mejor c√≥mo estos modelos aprenden.',
        paragraph3:
          'Poco despu√©s de la finalizaci√≥n de este curso, BAISH organizar√° un Hackath√≥n de Interpretabilidad Mecan√≠stica en FGV ‚Äî ¬°detalles por anunciar! Recomendamos encarecidamente que cualquier persona interesada en participar en el Hackath√≥n complete este curso como introducci√≥n al tema, lo que tambi√©n aumentar√° sus posibilidades de recibir un premio en la competencia.'
      },
      schedule: {
        title: 'Cronograma del Curso',
        session1: {
          title: 'Sesi√≥n 1 - Transformers e Interpretabilidad',
          date: '23 de agosto, viernes, a partir de las 14:30',
          location: 'Auditorio 537',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulos 2 y 3 del Manual',
          part1: '45 min: Introducci√≥n a transformers y atenci√≥n',
          part2: '20 min: Pausa para caf√©',
          part3: '45 min: Introducci√≥n a Interpretabilidad Mecan√≠stica',
          part4: '30 min: Programaci√≥n: PyTorch y TransformerLens',
          materialsTitle: 'Materiales:',
          handbookTitle: 'Manual:',
          papersTitle: 'Papers:',
          codingTitle: 'Programaci√≥n:',
          videosTitle: 'Videos:'
        },
        session2: {
          title: 'Sesi√≥n 2 - Circuitos',
          date: '30 de agosto, viernes, a partir de las 14:30',
          location: 'Auditorio 418',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulos 3 y 4 del Manual',
          part1: '40 min: Circuitos y el circuito de inducci√≥n',
          part2:
            '20 min: Exploraci√≥n del paper "Un Marco Matem√°tico para Circuitos Transformer"',
          part3: '20 min: Pausa para caf√©',
          part4: '60 min: Programaci√≥n: descubrimiento de circuitos'
        },
        session3: {
          title: 'Sesi√≥n 3 - Superposici√≥n',
          date: '6 de septiembre, viernes, a partir de las 14:30',
          location: 'Auditorio 537',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulo 5 del Manual',
          part1: '30 min: Superposici√≥n',
          part2:
            '30 min: Video de 3Blue1Brown "C√≥mo podr√≠an almacenar hechos los LLMs" y discusi√≥n',
          part3: '20 min: Pausa para caf√©',
          part4: '60 min: Programaci√≥n: superposici√≥n en modelos simplificados'
        },
        session4: {
          title: 'Sesi√≥n 4 - Autoencoders Dispersos (SAE)',
          date: '13 de septiembre, viernes, a partir de las 15:00',
          location: 'Auditorio 418',
          scheduleTitle: 'Programa:',
          preparationTitle: 'Preparaci√≥n:',
          preparation: 'Cap√≠tulo 6 del Manual',
          part1: '30 min: Autoencoders Dispersos (SAE) y exploraci√≥n pr√°ctica',
          part2:
            '30 min: Exploraci√≥n de los papers "Hacia la Monosem√°ntica: Descomponiendo Modelos de Lenguaje con Aprendizaje de Diccionario" y "Escalando Monosem√°ntica: Extrayendo Caracter√≠sticas Interpretables de Claude 3 Sonnet"',
          part3: '20 min: Pausa para caf√©',
          part4: '40 min: Programaci√≥n: Usando SAEs',
          part5:
            '30 min: ¬°Nuevas √°reas de exploraci√≥n y consejos para el Hackath√≥n!'
        }
      },
      resources: {
        title: 'Recursos Adicionales',
        opportunitiesTitle: 'Oportunidades',
        arenaTitle: 'ARENA',
        arenaDescription:
          'Oportunidad de pasar 4 semanas en Londres estudiando contenido pr√°ctico relevante para la investigaci√≥n en Seguridad de IA',
        arenaButton: 'Saber M√°s',
        matsTitle: 'MATS',
        matsDescription:
          'Programa para iniciar investigaci√≥n en Seguridad de IA con la orientaci√≥n de mentores con amplia experiencia en el campo. Varios autores de art√≠culos utilizados en este curso son mentores de MATS.',
        matsButton: 'Ver Mentores'
      }
    },

    // Handbook common elements
    handbook: {
      tableOfContents: '√çndice de Contenidos',
      prevChapter: '‚Üê Cap√≠tulo Anterior',
      nextChapter: 'Siguiente Cap√≠tulo ‚Üí',
      noNextChapter: 'Fin del Manual',
      furtherReading: 'Lecturas Adicionales',
      backToHome: '‚Üê Volver al Curso',

      // Chapter 4: Circuits
      chapter4: {
        title: 'Cap√≠tulo 4: Circuitos',
        subtitle:
          'Entendiendo la interpretabilidad basada en circuitos en redes neuronales',
        sections: {
          intro: 'Introducci√≥n a los Circuitos',
          defining: 'Definiendo Circuitos Neuronales',
          induction: 'El Circuito de Inducci√≥n',
          discovery: 'M√©todos de Descubrimiento de Circuitos',
          examples: 'Ejemplos Pr√°cticos',
          conclusion: 'Conclusi√≥n'
        },
        intro: {
          p1: 'El marco de "circuitos" es un concepto clave en interpretabilidad mecan√≠stica, ofreciendo una manera de entender c√≥mo las redes neuronales procesan informaci√≥n a trav√©s de componentes conectados que colectivamente implementan funciones espec√≠ficas.',
          p2: 'Este cap√≠tulo explora c√≥mo la interpretabilidad basada en circuitos nos ayuda a hacer ingenier√≠a inversa de los c√°lculos realizados por redes neuronales, con un enfoque en arquitecturas transformer utilizadas en modelos de lenguaje grandes.'
        },
        defining: {
          p1: 'Un circuito neuronal es un subconjunto de una red neuronal que implementa una funci√≥n espec√≠fica e identificable. Los circuitos consisten en neuronas (o grupos de neuronas) conectadas a trav√©s de capas que colectivamente realizan un c√°lculo.',
          p2: 'Las propiedades clave de los circuitos incluyen:',
          list1:
            'Modularidad: Los circuitos pueden estudiarse de forma aislada del resto de la red',
          list2:
            'Funcionalidad: Los circuitos implementan c√°lculos espec√≠ficos y significativos',
          list3:
            'Composici√≥n: Los comportamientos complejos emergen de combinaciones de circuitos m√°s simples',
          list4:
            'Representaci√≥n distribuida: La informaci√≥n a menudo se representa a trav√©s de m√∫ltiples neuronas en lugar de en neuronas individuales'
        },
        induction: {
          p1: 'El circuito de inducci√≥n es uno de los circuitos m√°s estudiados en modelos transformer. Este circuito permite a los modelos de lenguaje continuar patrones como "A, B, A, B, A, ?" con "B".',
          p2: 'El circuito funciona mediante:',
          step1:
            'Detectar cuando un token ha aparecido previamente en la secuencia',
          step2: 'Identificar qu√© token vino despu√©s en esa ocurrencia previa',
          step3: 'Predecir que el mismo token seguir√° en el contexto actual',
          p3: 'Este circuito involucra cabezas de atenci√≥n espec√≠ficas trabajando juntas a trav√©s de capas:',
          component1:
            'Detecci√≥n de token previo: Cabezas de atenci√≥n que buscan tokens repetidos',
          component2:
            'Copia de informaci√≥n: Mecanismos para recuperar lo que sigui√≥ a la ocurrencia previa',
          component3:
            'Proyecci√≥n de salida: Componentes que influyen en la predicci√≥n final'
        },
        discovery: {
          p1: 'Identificar circuitos dentro de redes neuronales implica varias t√©cnicas:',
          method1title: 'Parcheo de Activaciones',
          method1:
            'Esto implica reemplazar activaciones en ubicaciones espec√≠ficas de la red al procesar una entrada con activaciones de otra entrada, y luego observar c√≥mo cambia la salida. Ayuda a identificar qu√© componentes son causalmente importantes para una tarea determinada.',
          method2title: 'An√°lisis de Mediaci√≥n Causal',
          method2:
            'Este enfoque cuantifica cu√°nto contribuyen caminos espec√≠ficos en la red a una predicci√≥n particular, ayudando a identificar las conexiones m√°s importantes.',
          method3title: 'An√°lisis de Atenci√≥n',
          method3:
            'Examinar patrones de atenci√≥n puede revelar qu√© partes de la secuencia de entrada influyen en otras partes, proporcionando informaci√≥n sobre el flujo de informaci√≥n.'
        },
        examples: {
          p1: 'M√°s all√° de la inducci√≥n, los investigadores han identificado varios otros circuitos:',
          circuit1:
            'Identificaci√≥n de Objetos Indirectos: Circuitos que rastrean relaciones entre sujetos, verbos y objetos en oraciones',
          circuit2:
            'Circuitos de Negaci√≥n: Componentes que detectan y procesan la negaci√≥n en el texto',
          circuit3:
            'Circuitos de Movimiento de Nombres: Mecanismos para rastrear y referenciar entidades a lo largo de un texto',
          p2: 'Estos ejemplos demuestran c√≥mo la comprensi√≥n compleja del lenguaje emerge de implementaciones espec√≠ficas de circuitos.'
        },
        conclusion: {
          p1: 'El marco de circuitos proporciona un enfoque poderoso para entender c√≥mo las redes neuronales implementan funciones espec√≠ficas. Al identificar y estudiar estos circuitos, los investigadores pueden obtener informaci√≥n sobre c√≥mo los modelos procesan informaci√≥n y hacen predicciones.',
          p2: 'Esta comprensi√≥n es crucial para mejorar la interpretabilidad, aumentar la seguridad del modelo y desarrollar sistemas de IA m√°s confiables. El pr√≥ximo cap√≠tulo explorar√° c√≥mo las redes neuronales manejan la informaci√≥n cuando tienen capacidad limitada a trav√©s del concepto de superposici√≥n.'
        },
        reading1: 'Aprendizaje en Contexto y Cabezas de Inducci√≥n',
        reading2: 'Un Marco Matem√°tico para Circuitos Transformer',
        reading3: 'Acercamiento: Una Introducci√≥n a los Circuitos'
      },

      // Chapter 5: Superposition
      chapter5: {
        title: 'Cap√≠tulo 5: Superposici√≥n',
        subtitle:
          'Entendiendo c√≥mo las redes neuronales representan m√°s caracter√≠sticas de las que tienen dimensiones',
        sections: {
          intro: 'Introducci√≥n a la Superposici√≥n',
          competition: 'Competencia de Caracter√≠sticas',
          polysemanticity: 'Polisem√°ntica',
          toyModels: 'Modelos Simplificados de Superposici√≥n',
          implications: 'Implicaciones para la Interpretabilidad',
          conclusion: 'Conclusi√≥n'
        },
        intro: {
          p1: 'La superposici√≥n se refiere al fen√≥meno donde las redes neuronales representan m√°s caracter√≠sticas de las que tienen neuronas o dimensiones. Esto ocurre porque las redes a menudo necesitan rastrear muchas m√°s caracter√≠sticas de las que tienen par√°metros disponibles para representarlas individualmente.',
          p2: 'El concepto fue formalizado en el art√≠culo "Modelos Simplificados de Superposici√≥n" por Anthropic, que demostr√≥ c√≥mo las redes pueden codificar m√∫ltiples caracter√≠sticas en un espacio de menor dimensi√≥n aprovechando la geometr√≠a de los patrones de co-ocurrencia de caracter√≠sticas.'
        },
        competition: {
          p1: 'Cuando una red neuronal tiene menos dimensiones que las caracter√≠sticas que necesita representar, estas caracter√≠sticas deben "competir" por el espacio de representaci√≥n. La red aprende a asignar su capacidad de representaci√≥n limitada de manera eficiente.',
          p2: 'Los factores clave que influyen en qu√© caracter√≠sticas se representan incluyen:',
          list1:
            'Frecuencia: Las caracter√≠sticas m√°s comunes tienen m√°s probabilidades de ser representadas',
          list2:
            'Importancia: Se priorizan las caracter√≠sticas que afectan fuertemente a la funci√≥n de p√©rdida',
          list3:
            'Correlaci√≥n: Las caracter√≠sticas que a menudo co-ocurren pueden compartir espacio de representaci√≥n',
          list4:
            'Ortogonalidad: Las caracter√≠sticas que pueden representarse en direcciones ortogonales son m√°s f√°ciles de separar'
        },
        polysemanticity: {
          p1: 'La polisem√°ntica es una consecuencia directa de la superposici√≥n. Se refiere al fen√≥meno donde neuronas individuales o direcciones de la red responden a m√∫ltiples caracter√≠sticas no relacionadas.',
          p2: 'En una red polisem√°ntica:',
          list1:
            'Las neuronas individuales pueden responder a m√∫ltiples conceptos sem√°nticamente distintos',
          list2:
            'Las representaciones de caracter√≠sticas se distribuyen a trav√©s de muchas neuronas',
          list3:
            'Puede no haber una correspondencia clara uno a uno entre los componentes de la red y los conceptos interpretables por humanos',
          p3: 'Esto hace que la interpretaci√≥n sea desafiante, ya que no podemos simplemente analizar neuronas individuales para entender lo que la red est√° representando.'
        },
        toyModels: {
          p1: 'Los investigadores han desarrollado modelos simplificados para estudiar la superposici√≥n. Estos modelos ayudan a ilustrar c√≥mo las redes pueden incorporar muchas caracter√≠sticas en espacios de dimensiones m√°s bajas.',
          p2: 'Un modelo simplificado t√≠pico podr√≠a involucrar:',
          step1:
            'Generar datos sint√©ticos con un n√∫mero conocido de caracter√≠sticas dispersas',
          step2: 'Entrenar un modelo con menos dimensiones que caracter√≠sticas',
          step3:
            'Analizar c√≥mo el modelo representa estas caracter√≠sticas en su espacio limitado',
          p3: 'Estos experimentos han revelado que las redes pueden usar disposiciones geom√©tricas inteligentes para codificar caracter√≠sticas de manera eficiente, a menudo aprovechando propiedades como la dispersi√≥n (caracter√≠sticas que rara vez aparecen simult√°neamente).'
        },
        implications: {
          p1: 'La superposici√≥n plantea varios desaf√≠os para la investigaci√≥n de interpretabilidad:',
          list1:
            'El an√°lisis directo de neuronas puede revelar informaci√≥n enga√±osa o incompleta',
          list2:
            'Las caracter√≠sticas pueden estar codificadas en patrones complejos distribuidos a trav√©s de muchas neuronas',
          list3:
            'Las t√©cnicas simples de sondeo lineal pueden fallar en detectar caracter√≠sticas importantes',
          p2: 'Sin embargo, entender la superposici√≥n tambi√©n ofrece oportunidades:',
          list4:
            'Sugiere enfocarse en encontrar la base correcta para el an√°lisis, en lugar de examinar neuronas individuales',
          list5:
            'T√©cnicas como los Autoencoders Dispersos (cubiertos en el pr√≥ximo cap√≠tulo) pueden ayudar a extraer caracter√≠sticas de representaciones superpuestas',
          list6:
            'El conocimiento de los patrones de superposici√≥n puede informar un mejor dise√±o y entrenamiento de arquitecturas'
        },
        conclusion: {
          p1: 'La superposici√≥n es una propiedad fundamental de las redes neuronales que surge cuando necesitan representar m√°s caracter√≠sticas de las que tienen dimensiones. Esto lleva a neuronas polisem√°nticas y representaciones distribuidas que complican los esfuerzos de interpretabilidad.',
          p2: 'Entender la superposici√≥n es esencial para desarrollar m√©todos efectivos para interpretar redes neuronales, especialmente modelos de lenguaje grandes con miles de millones de par√°metros que rastrean potencialmente billones de caracter√≠sticas. El pr√≥ximo cap√≠tulo explorar√° c√≥mo los Autoencoders Dispersos pueden ayudar a abordar este desaf√≠o desenredando estas representaciones superpuestas.'
        },
        reading1: 'Modelos Simplificados de Superposici√≥n (Anthropic)',
        reading2:
          'Medidas de Progreso para la Comprensi√≥n Profunda mediante Interpretabilidad Mecan√≠stica',
        reading3: '3Blue1Brown: ¬øC√≥mo los LLMs podr√≠an almacenar hechos?'
      },

      // Chapter 6: Sparse Autoencoders
      chapter6: {
        title: 'Cap√≠tulo 6: Autoencoders Dispersos',
        subtitle:
          'Usando Autoencoders Dispersos para desenredar la superposici√≥n en redes neuronales',
        sections: {
          intro: 'Introducci√≥n a los Autoencoders Dispersos',
          architecture: 'Arquitectura y Entrenamiento',
          features: 'Extracci√≥n de Caracter√≠sticas',
          applications: 'Aplicaciones en Interpretabilidad',
          research: 'Investigaci√≥n Reciente',
          conclusion: 'Conclusi√≥n'
        },
        intro: {
          p1: 'Como vimos en el cap√≠tulo anterior, las redes neuronales a menudo representan informaci√≥n de manera superpuesta, con muchas caracter√≠sticas compartiendo las mismas neuronas o dimensiones. Esta polisem√°ntica hace que la interpretaci√≥n sea desafiante. Los Autoencoders Dispersos (SAEs) son una herramienta poderosa dise√±ada para abordar este desaf√≠o desenredando estas representaciones superpuestas.',
          p2: 'El objetivo de un SAE es transformar las representaciones polisem√°nticas y distribuidas en una red neuronal en una representaci√≥n monosem√°ntica y dispersa, donde cada caracter√≠stica corresponde a un concepto espec√≠fico e interpretable.'
        },
        architecture: {
          p1: 'Un Autoencoder Disperso consiste en:',
          list1:
            'Una red codificadora que mapea las activaciones de la red neuronal original a un espacio disperso de mayor dimensi√≥n',
          list2:
            'Una red decodificadora que reconstruye las activaciones originales a partir de esta representaci√≥n dispersa',
          p2: 'Las restricciones clave en el entrenamiento de un SAE son:',
          constraint1:
            'P√©rdida de reconstrucci√≥n: El decodificador debe reconstruir con precisi√≥n las activaciones originales',
          constraint2:
            'Restricci√≥n de dispersi√≥n: Cada entrada debe activar solo un peque√±o n√∫mero de caracter√≠sticas en la representaci√≥n codificada',
          p3: 'Estas restricciones aseguran que el SAE aprenda un diccionario de caracter√≠sticas interpretables que pueden activarse individualmente o en peque√±os grupos para representar los estados internos de la red.'
        },
        features: {
          p1: 'Despu√©s de entrenar un SAE en un gran conjunto de datos de activaciones de una red neuronal, podemos analizar las caracter√≠sticas que ha aprendido:',
          list1:
            'Cada caracter√≠stica puede visualizarse examinando qu√© tipos de entradas la activan m√°s fuertemente',
          list2:
            'Las caracter√≠sticas pueden nombrarse seg√∫n los patrones que reconocen (por ejemplo, "detector de comillas" u "operador de multiplicaci√≥n")',
          list3:
            'El diccionario de caracter√≠sticas proporciona una nueva base para entender las representaciones internas de la red',
          p2: 'A diferencia del an√°lisis de neuronas individuales, las caracter√≠sticas del SAE a menudo corresponden a conceptos significativos e interpretables por humanos porque est√°n dise√±adas para desenredar las representaciones superpuestas.'
        },
        applications: {
          p1: 'Los Autoencoders Dispersos pueden usarse para varias tareas de interpretabilidad:',
          task1title: 'Descubrimiento de Circuitos',
          task1:
            'Al rastrear qu√© caracter√≠sticas del SAE se activan en respuesta a entradas espec√≠ficas, los investigadores pueden identificar los circuitos computacionales dentro de la red.',
          task2title: 'Atribuci√≥n de Caracter√≠sticas',
          task2:
            'Los SAEs pueden ayudar a determinar qu√© caracter√≠sticas contribuyen a predicciones espec√≠ficas, proporcionando informaci√≥n sobre c√≥mo el modelo toma decisiones.',
          task3title: 'Edici√≥n del Comportamiento del Modelo',
          task3:
            'Una vez que se identifican caracter√≠sticas interpretables, es posible modificar el comportamiento del modelo interviniendo en caracter√≠sticas espec√≠ficas, potencialmente habilitando sistemas de IA m√°s seguros.'
        },
        research: {
          p1: 'Los avances recientes en la investigaci√≥n de SAE incluyen:',
          advance1:
            'Escalar SAEs a modelos m√°s grandes, como Claude 3 Sonnet de Anthropic',
          advance2:
            'Mejorar las t√©cnicas de entrenamiento para identificar caracter√≠sticas m√°s interpretables',
          advance3:
            'Desarrollar m√©todos autom√°ticos para nombrar y categorizar las caracter√≠sticas descubiertas',
          advance4:
            'Usar SAEs para entender abstracciones de nivel superior en modelos de lenguaje',
          p2: 'Art√≠culos como "Hacia la Monosem√°ntica: Descomponiendo Modelos de Lenguaje con Aprendizaje de Diccionario" y "Escalando la Monosem√°ntica: Extrayendo Caracter√≠sticas Interpretables de Claude 3 Sonnet" demuestran el potencial de los SAEs para entender modelos cada vez m√°s complejos.'
        },
        conclusion: {
          p1: 'Los Autoencoders Dispersos representan uno de los enfoques m√°s prometedores para abordar el problema de superposici√≥n en redes neuronales. Al transformar representaciones polisem√°nticas en caracter√≠sticas monosem√°nticas, los SAEs proporcionan una herramienta poderosa para la interpretabilidad mecan√≠stica.',
          p2: 'A medida que la investigaci√≥n en esta √°rea contin√∫a avanzando, los SAEs pueden jugar un papel crucial en la construcci√≥n de sistemas de IA m√°s transparentes, confiables y alineables. Entender el funcionamiento interno de las redes neuronales no es solo una b√∫squeda acad√©mica sino una necesidad pr√°ctica a medida que estos sistemas se vuelven cada vez m√°s poderosos e integrados en nuestra sociedad.'
        },
        reading1:
          'Hacia la Monosem√°ntica: Descomponiendo Modelos de Lenguaje con Aprendizaje de Diccionario',
        reading2:
          'Escalando la Monosem√°ntica: Extrayendo Caracter√≠sticas Interpretables de Claude 3 Sonnet',
        reading3: 'SolidGoldMagikarp e Identificaci√≥n de Objetos Indirectos'
      }
    }
  }
}
